# Agent Flow Lite ä»£ç å®¡æŸ¥æŠ¥å‘Š

> å®¡æŸ¥æ—¥æœŸ: 2026-02-03
> å®¡æŸ¥èŒƒå›´: å…¨éƒ¨æ ¸å¿ƒä»£ç ï¼ˆå‰ç«¯ + åç«¯ï¼‰
> å®¡æŸ¥ç›®æ ‡: è¯†åˆ«é€»è¾‘æ¼æ´ã€åŠŸèƒ½ç¼ºé™·ã€å®‰å…¨é—®é¢˜ã€ä¼˜åŒ–ç©ºé—´

---

## ç›®å½•

1. [é¡¹ç›®è®¾è®¡ç›®æ ‡å›é¡¾](#é¡¹ç›®è®¾è®¡ç›®æ ‡å›é¡¾)
2. [ä¸¥é‡é€»è¾‘æ¼æ´ï¼ˆP0ï¼‰](#ä¸€ä¸¥é‡é€»è¾‘æ¼æ´p0)
3. [ä¸­ç­‰é—®é¢˜ï¼ˆP1ï¼‰](#äºŒä¸­ç­‰é—®é¢˜p1)
4. [å¹¶å‘ä¸æ€§èƒ½é—®é¢˜ï¼ˆP2ï¼‰](#ä¸‰å¹¶å‘ä¸æ€§èƒ½é—®é¢˜p2)
5. [å®‰å…¨é—®é¢˜](#å››å®‰å…¨é—®é¢˜)
6. [æ¶æ„è®¾è®¡é—®é¢˜](#äº”æ¶æ„è®¾è®¡é—®é¢˜)
7. [è¯¦ç»†ä¼˜åŒ–æ–¹æ¡ˆ](#å…­è¯¦ç»†ä¼˜åŒ–æ–¹æ¡ˆ)
8. [ä¼˜å…ˆçº§æ’åºä¸å®æ–½è·¯çº¿å›¾](#ä¸ƒä¼˜å…ˆçº§æ’åºä¸å®æ–½è·¯çº¿å›¾)

---

## é¡¹ç›®è®¾è®¡ç›®æ ‡å›é¡¾

Agent Flow Lite çš„æ ¸å¿ƒæ„¿æ™¯æ˜¯ï¼š

- **å¯è§†åŒ–å·¥ä½œæµç¼–æ’** - ç”¨æˆ·é€šè¿‡æ‹–æ‹½èŠ‚ç‚¹ã€è¿çº¿çš„æ–¹å¼è®¾è®¡ AI å¤„ç†æµç¨‹
- **æ™ºèƒ½ RAG å¯¹è¯** - åŸºäºçŸ¥è¯†åº“çš„å¢å¼ºæ£€ç´¢é—®ç­”
- **å·¥ä½œæµé©±åŠ¨çš„å¯¹è¯** - Chat æŒ‰ç…§å·¥ä½œæµå®šä¹‰çš„é€»è¾‘æ‰§è¡Œ

**æ ¸å¿ƒå‘ç°**: å½“å‰å®ç°ä¸è®¾è®¡ç›®æ ‡å­˜åœ¨ä¸¥é‡åç¦»ï¼Œå·¥ä½œæµç¼–è¾‘å™¨æ˜¯"ç”»å›¾å·¥å…·"ï¼ŒChat æ˜¯"ç‹¬ç«‹çš„ RAG å¯¹è¯"ï¼Œä¸¤è€…**å®Œå…¨æ²¡æœ‰æ‰“é€š**ã€‚

---

## ä¸€ã€ä¸¥é‡é€»è¾‘æ¼æ´ï¼ˆP0ï¼‰

### 1.1 å·¥ä½œæµæ‰§è¡Œå¼•æ“å®Œå…¨ç¼ºå¤±

**é—®é¢˜æè¿°**

è®¾è®¡äº†å®Œæ•´çš„å·¥ä½œæµç¼–è¾‘å™¨ï¼ˆæ”¯æŒ 5 ç§èŠ‚ç‚¹ç±»å‹ã€è¿çº¿ã€é…ç½®é¢æ¿ï¼‰ï¼Œä½†åç«¯**æ²¡æœ‰ä»»ä½•ä»£ç æ‰§è¡Œå·¥ä½œæµ**ã€‚ç”¨æˆ·ç”»å¥½çš„å·¥ä½œæµåªèƒ½ä¿å­˜ä¸º JSON æ•°æ®ï¼Œæ°¸è¿œä¸ä¼šè¢«æ‰§è¡Œã€‚

**å½±å“èŒƒå›´**

- `backend/app/api/workflow.py` - åªæœ‰ CRUDï¼Œæ²¡æœ‰æ‰§è¡Œ
- `frontend/src/views/WorkflowEditor.vue` - åªèƒ½ç¼–è¾‘ï¼Œä¸èƒ½è¿è¡Œ

**å½“å‰è¡Œä¸º vs æœŸæœ›è¡Œä¸º**

```
å½“å‰è¡Œä¸º:
ç”¨æˆ·ç”»å·¥ä½œæµ â†’ ä¿å­˜åˆ° workflows.json â†’ ç»“æŸï¼ˆæ­»æ•°æ®ï¼‰

æœŸæœ›è¡Œä¸º:
ç”¨æˆ·ç”»å·¥ä½œæµ â†’ ä¿å­˜ â†’ åœ¨ Chat ä¸­é€‰æ‹©å·¥ä½œæµ â†’ æŒ‰æ‹“æ‰‘é¡ºåºæ‰§è¡Œå„èŠ‚ç‚¹ â†’ è¿”å›ç»“æœ
```

**ç¼ºå¤±çš„æ ¸å¿ƒç»„ä»¶**

```python
# éœ€è¦å®ç°: backend/app/core/workflow_engine.py

class WorkflowEngine:
    """å·¥ä½œæµæ‰§è¡Œå¼•æ“"""

    def __init__(self, workflow: Workflow):
        self.workflow = workflow
        self.graph = self._build_graph()
        self.step_outputs = {}  # å­˜å‚¨å„èŠ‚ç‚¹çš„è¾“å‡º

    def _build_graph(self) -> dict:
        """ä» nodes/edges æ„å»ºé‚»æ¥è¡¨"""
        pass

    def _topological_sort(self) -> List[str]:
        """æ‹“æ‰‘æ’åºç¡®å®šæ‰§è¡Œé¡ºåº"""
        pass

    def _execute_node(self, node_id: str, input_data: Any) -> Any:
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        node = self._get_node(node_id)
        node_type = node["type"]

        if node_type == "start":
            return self._execute_start_node(node, input_data)
        elif node_type == "llm":
            return self._execute_llm_node(node, input_data)
        elif node_type == "knowledge":
            return self._execute_knowledge_node(node, input_data)
        elif node_type == "condition":
            return self._execute_condition_node(node, input_data)
        elif node_type == "end":
            return self._execute_end_node(node, input_data)

    async def execute(self, initial_input: str) -> dict:
        """æ‰§è¡Œæ•´ä¸ªå·¥ä½œæµ"""
        execution_order = self._topological_sort()
        current_data = initial_input

        for node_id in execution_order:
            current_data = await self._execute_node(node_id, current_data)
            self.step_outputs[node_id] = current_data

        return self.step_outputs
```

---

### 1.2 Chat ä¸ Workflow å®Œå…¨å‰²è£‚

**é—®é¢˜æè¿°**

`ChatTerminal.vue` å‘é€æ¶ˆæ¯æ—¶**ä»ä¸ä¼ é€’ workflow_id**ï¼Œå³ä½¿åç«¯ API æ”¯æŒè¯¥å‚æ•°ï¼Œä¹Ÿä»æœªè¢«ä½¿ç”¨ã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```typescript
// frontend/src/views/ChatTerminal.vue:207-218
async function connectSSE(sessionId: string, message: string) {
  const response = await fetch('/api/v1/chat/completions', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      session_id: sessionId,
      message: message,
      // ç¼ºå¤±: workflow_id
      // ç¼ºå¤±: kb_id
    }),
  })
  // ...
}
```

```python
# backend/app/api/chat.py:177-195
@router.post("/completions")
async def chat_completions(request: ChatRequest) -> StreamingResponse:
    # request.workflow_id è¢«æ¥æ”¶ä½†ä»æœªä½¿ç”¨
    # æ²¡æœ‰ä»»ä½•ä»£ç è°ƒç”¨å·¥ä½œæµæ‰§è¡Œå¼•æ“
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

```typescript
// å‰ç«¯ - æ·»åŠ å·¥ä½œæµå’ŒçŸ¥è¯†åº“é€‰æ‹©
const selectedWorkflowId = ref<string>('')
const selectedKbId = ref<string>('')

async function connectSSE(sessionId: string, message: string) {
  const response = await fetch('/api/v1/chat/completions', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      session_id: sessionId,
      message: message,
      workflow_id: selectedWorkflowId.value || undefined,
      kb_id: selectedKbId.value || undefined,
    }),
  })
}
```

```python
# åç«¯ - æ ¹æ® workflow_id å†³å®šæ‰§è¡Œé€»è¾‘
@router.post("/completions")
async def chat_completions(request: ChatRequest) -> StreamingResponse:
    if request.workflow_id:
        # ä½¿ç”¨å·¥ä½œæµå¼•æ“æ‰§è¡Œ
        workflow = await get_workflow(request.workflow_id)
        engine = WorkflowEngine(workflow)
        result = await engine.execute(request.message)
        # æµå¼è¿”å›æ‰§è¡Œè¿‡ç¨‹å’Œç»“æœ
    else:
        # æ™®é€š RAG å¯¹è¯ï¼ˆå½“å‰å®ç°ï¼‰
        pass
```

---

### 1.3 æ¡ä»¶èŠ‚ç‚¹è¡¨è¾¾å¼æ— æ³•æ‰§è¡Œ

**é—®é¢˜æè¿°**

æ¡ä»¶èŠ‚ç‚¹è®©ç”¨æˆ·å¡«å†™ JavaScript è¡¨è¾¾å¼ï¼ˆå¦‚ `{{step1.output}} === 'yes'`ï¼‰ï¼Œä½†ï¼š

1. åç«¯æ˜¯ Pythonï¼Œæ— æ³•ç›´æ¥æ‰§è¡Œ JavaScript
2. æ²¡æœ‰å®ç°å˜é‡å¼•ç”¨è§£æï¼ˆ`{{step1.output}}` æ›¿æ¢é€»è¾‘ï¼‰
3. å³ä½¿å®ç°äº†ï¼Œç”¨ `eval()` ä¼šæœ‰ä¸¥é‡å®‰å…¨é—®é¢˜

**é—®é¢˜ä»£ç ä½ç½®**

```vue
<!-- frontend/src/components/NodeConfigPanel.vue:81-93 -->
<div v-if="nodeType === 'condition'" class="config-section">
  <textarea
    v-model="config.expression"
    placeholder="ä¾‹å¦‚: {{step1.output}} === 'yes'"
  ></textarea>
  <small>ä½¿ç”¨ {{stepId.output}} å¼•ç”¨å…¶ä»–èŠ‚ç‚¹çš„è¾“å‡º</small>
</div>
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

æ–¹æ¡ˆ A: ä½¿ç”¨ç»“æ„åŒ–æ¡ä»¶ï¼ˆæ¨èï¼‰

```python
# å®šä¹‰ç»“æ„åŒ–æ¡ä»¶æ¨¡å‹
class ConditionRule(BaseModel):
    left_operand: str      # å˜é‡å¼•ç”¨ï¼Œå¦‚ "step1.output"
    operator: str          # "equals", "contains", "greater_than" ç­‰
    right_operand: str     # æ¯”è¾ƒå€¼

class ConditionConfig(BaseModel):
    rules: List[ConditionRule]
    logic: str = "and"     # "and" æˆ– "or"

# æ‰§è¡Œæ¡ä»¶åˆ¤æ–­
def evaluate_condition(config: ConditionConfig, step_outputs: dict) -> bool:
    results = []
    for rule in config.rules:
        left_value = resolve_variable(rule.left_operand, step_outputs)
        right_value = rule.right_operand
        result = compare(left_value, rule.operator, right_value)
        results.append(result)

    if config.logic == "and":
        return all(results)
    return any(results)
```

æ–¹æ¡ˆ B: ä½¿ç”¨å®‰å…¨è¡¨è¾¾å¼å¼•æ“

```python
# ä½¿ç”¨ simpleeval åº“ï¼ˆéœ€è¦å®‰è£…ï¼‰
from simpleeval import simple_eval

def evaluate_expression(expression: str, step_outputs: dict) -> bool:
    # æ›¿æ¢å˜é‡å¼•ç”¨
    for step_id, output in step_outputs.items():
        expression = expression.replace(
            f"{{{{{step_id}.output}}}}",
            repr(output)
        )

    # å®‰å…¨æ‰§è¡Œï¼ˆsimpleeval åªå…è®¸åŸºç¡€æ“ä½œï¼‰
    return simple_eval(expression)
```

---

### 1.4 åˆ é™¤æ–‡æ¡£æ—¶å‘é‡æ•°æ®æ®‹ç•™

**é—®é¢˜æè¿°**

åˆ é™¤æ–‡æ¡£æ—¶ï¼ŒChromaDB ä¸­çš„å‘é‡æ•°æ®æ²¡æœ‰è¢«æ­£ç¡®åˆ é™¤ã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```python
# backend/app/core/chroma_client.py:117-133
def delete_document(self, kb_id: str, document_id: str) -> bool:
    try:
        collection = self._client.get_collection(name=f"kb_{kb_id}")
        collection.delete(ids=[document_id])  # é”™è¯¯ï¼
        return True
    except ValueError:
        return False
```

**é—®é¢˜åˆ†æ**

æ–‡æ¡£å­˜å‚¨æ—¶ä½¿ç”¨çš„ ID æ ¼å¼æ˜¯ `{doc_id}_chunk_0`, `{doc_id}_chunk_1` ç­‰ï¼š

```python
# backend/app/core/rag.py:131
ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
```

ä½†åˆ é™¤æ—¶åªä¼ äº† `doc_id`ï¼Œæ— æ³•åŒ¹é…ä»»ä½•è®°å½•ã€‚

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
# æ–¹æ¡ˆ 1: ä½¿ç”¨ metadata è¿‡æ»¤åˆ é™¤
def delete_document(self, kb_id: str, document_id: str) -> bool:
    try:
        collection = self._client.get_collection(name=f"kb_{kb_id}")
        # ä½¿ç”¨ where æ¡ä»¶æŒ‰ doc_id å…ƒæ•°æ®åˆ é™¤æ‰€æœ‰ chunks
        collection.delete(where={"doc_id": document_id})
        return True
    except Exception as e:
        print(f"Delete failed: {e}")
        return False

# æ–¹æ¡ˆ 2: å…ˆæŸ¥è¯¢å†åˆ é™¤
def delete_document(self, kb_id: str, document_id: str) -> bool:
    try:
        collection = self._client.get_collection(name=f"kb_{kb_id}")
        # æŸ¥è¯¢æ‰€æœ‰å±äºè¯¥æ–‡æ¡£çš„ chunk IDs
        results = collection.get(
            where={"doc_id": document_id},
            include=[]
        )
        if results["ids"]:
            collection.delete(ids=results["ids"])
        return True
    except Exception as e:
        return False
```

---

### 1.5 RAG æœç´¢æ‰§è¡Œäº†ä¸¤æ¬¡

**é—®é¢˜æè¿°**

æ¯æ¬¡èŠå¤©è¯·æ±‚ä¸­ï¼ŒRAG æœç´¢è¢«è°ƒç”¨äº†ä¸¤æ¬¡ï¼š
1. åœ¨ `chat_completions` ä¸­æ„å»º system prompt æ—¶
2. åœ¨ `chat_stream_generator` ä¸­å‘é€æ€ç»´é“¾äº‹ä»¶æ—¶

**é—®é¢˜ä»£ç ä½ç½®**

```python
# backend/app/api/chat.py:229-240 - ç¬¬ä¸€æ¬¡æœç´¢
if request.kb_id:
    try:
        rag_pipeline = get_rag_pipeline()
        results = rag_pipeline.search(request.kb_id, request.message, top_k=5)
        # ...ç”¨äºæ„å»º system prompt

# backend/app/api/chat.py:103-114 (chat_stream_generator) - ç¬¬äºŒæ¬¡æœç´¢
if request.kb_id:
    rag_pipeline = get_rag_pipeline()
    retrieved_results = rag_pipeline.search(
        request.kb_id, request.message, top_k=5
    )
    # ...ç”¨äºå‘é€æ€ç»´é“¾äº‹ä»¶
```

**å½±å“**

- Embedding API è°ƒç”¨æ¬¡æ•°ç¿»å€
- å“åº”å»¶è¿Ÿå¢åŠ 
- æˆæœ¬å¢åŠ 

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
@router.post("/completions")
async def chat_completions(request: ChatRequest) -> StreamingResponse:
    # åªæœç´¢ä¸€æ¬¡ï¼Œç»“æœä¼ é€’ç»™ generator
    retrieved_results = []
    retrieved_context = None

    if request.kb_id:
        try:
            rag_pipeline = get_rag_pipeline()
            retrieved_results = rag_pipeline.search(
                request.kb_id, request.message, top_k=5
            )
            if retrieved_results:
                context_parts = [f"[{i}] {r['text']}" for i, r in enumerate(retrieved_results[:3], 1)]
                retrieved_context = "\n\n".join(context_parts)
        except Exception:
            pass

    system_prompt = build_system_prompt(bool(request.kb_id), retrieved_context)
    # ...

    # å°†æœç´¢ç»“æœä¼ é€’ç»™ generatorï¼Œä¸å†é‡å¤æœç´¢
    async def stream_with_save():
        async for chunk in chat_stream_generator(
            request,
            messages_for_llm,
            pre_retrieved_results=retrieved_results  # ä¼ é€’å·²æœ‰ç»“æœ
        ):
            yield chunk
```

---

## äºŒã€ä¸­ç­‰é—®é¢˜ï¼ˆP1ï¼‰

### 2.1 ä¿å­˜å·¥ä½œæµä¸¢å¤±èŠ‚ç‚¹é…ç½®æ•°æ®

**é—®é¢˜æè¿°**

ç”¨æˆ·åœ¨èŠ‚ç‚¹é…ç½®é¢æ¿ä¸­è®¾ç½®çš„å‚æ•°ï¼ˆsystemPrompt, temperature, knowledgeBaseId ç­‰ï¼‰åœ¨ä¿å­˜å·¥ä½œæµæ—¶è¢«ä¸¢å¼ƒã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```typescript
// frontend/src/views/WorkflowEditor.vue:217-233
async function saveWorkflow() {
  const flowData = toObject()
  const response = await axios.post(`${API_BASE}/workflows`, {
    name: workflowName,
    graph_data: {
      nodes: flowData.nodes.map((n: any) => ({
        id: n.id,
        type: n.type,
        position: n.position,
        label: n.label
        // ç¼ºå¤±: data: n.data  <-- èŠ‚ç‚¹é…ç½®æ•°æ®åœ¨è¿™é‡Œï¼
      })),
      edges: flowData.edges.map((e: any) => ({
        id: e.id,
        source: e.source,
        target: e.target
      }))
    }
  })
}
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

```typescript
async function saveWorkflow() {
  const flowData = toObject()
  const response = await axios.post(`${API_BASE}/workflows`, {
    name: workflowName,
    description: '',
    graph_data: {
      nodes: flowData.nodes.map((n: any) => ({
        id: n.id,
        type: n.type,
        position: n.position,
        label: n.label,
        data: n.data || {}  // æ·»åŠ è¿™ä¸€è¡Œï¼
      })),
      edges: flowData.edges.map((e: any) => ({
        id: e.id,
        source: e.source,
        target: e.target,
        sourceHandle: e.sourceHandle,  // å¯é€‰ï¼šä¿å­˜è¿æ¥ç‚¹ä¿¡æ¯
        targetHandle: e.targetHandle
      }))
    }
  })
}
```

åŒæ—¶ä¿®æ”¹åŠ è½½é€»è¾‘ï¼š

```typescript
// frontend/src/views/WorkflowEditor.vue:265-271
if (graphData && graphData.nodes) {
  setNodes(graphData.nodes.map((n: any) => ({
    id: n.id,
    type: n.type,
    position: n.position,
    label: n.label || getDefaultLabel(n.type),
    data: n.data || {}  // ç¡®ä¿åŠ è½½æ—¶ä¹ŸåŒ…å« data
  })))
}
```

---

### 2.2 ä¼šè¯å†å²å‰åç«¯ä¸åŒæ­¥

**é—®é¢˜æè¿°**

- å‰ç«¯ `sessions` æ•°ç»„å­˜å‚¨åœ¨ç»„ä»¶å†…å­˜ä¸­
- åç«¯ä¼šè¯å­˜å‚¨åœ¨ `data/sessions/{session_id}.json`
- ä¸¤è€…ç‹¬ç«‹ï¼Œæ²¡æœ‰åŒæ­¥æœºåˆ¶

**é—®é¢˜è¡¨ç°**

1. åˆ·æ–°é¡µé¢åï¼Œå‰ç«¯ä¼šè¯åˆ—è¡¨æ¸…ç©º
2. åç«¯æ•°æ®ä»ç„¶å­˜åœ¨ï¼Œä½†å‰ç«¯çœ‹ä¸åˆ°
3. æ–°å»ºä¼šè¯ ID å¯èƒ½ä¸å·²æœ‰åç«¯æ•°æ®å†²çª

**ä¼˜åŒ–æ–¹æ¡ˆ**

æ–¹æ¡ˆ A: å¯åŠ¨æ—¶ä»åç«¯åŠ è½½ä¼šè¯åˆ—è¡¨

```typescript
// æ·»åŠ è·å–ä¼šè¯åˆ—è¡¨çš„ API
// åç«¯: GET /api/v1/chat/sessions

// å‰ç«¯å¯åŠ¨æ—¶åŠ è½½
onMounted(async () => {
  try {
    const response = await axios.get('/api/v1/chat/sessions')
    sessions.value = response.data.sessions.map((s: any) => ({
      id: s.session_id,
      title: s.title || 'ä¼šè¯ ' + s.session_id.slice(0, 8),
      createdAt: new Date(s.created_at).getTime(),
      updatedAt: new Date(s.updated_at).getTime(),
      messages: s.messages || []
    }))

    if (sessions.value.length > 0) {
      currentSessionId.value = sessions.value[0].id
    } else {
      createNewSession()
    }
  } catch (error) {
    console.error('åŠ è½½ä¼šè¯åˆ—è¡¨å¤±è´¥:', error)
    createNewSession()
  }
})
```

æ–¹æ¡ˆ B: ä½¿ç”¨ LocalStorage æŒä¹…åŒ–å‰ç«¯çŠ¶æ€

```typescript
// ä¿å­˜ä¼šè¯åˆ° LocalStorage
watch(sessions, (newSessions) => {
  localStorage.setItem('chat_sessions', JSON.stringify(newSessions))
}, { deep: true })

// å¯åŠ¨æ—¶æ¢å¤
onMounted(() => {
  const saved = localStorage.getItem('chat_sessions')
  if (saved) {
    sessions.value = JSON.parse(saved)
    if (sessions.value.length > 0) {
      currentSessionId.value = sessions.value[0].id
    }
  }
  if (sessions.value.length === 0) {
    createNewSession()
  }
})
```

---

### 2.3 çŸ¥è¯†åº“åˆ é™¤ API ä¸å­˜åœ¨

**é—®é¢˜æè¿°**

æœ‰åˆ›å»ºçŸ¥è¯†åº“çš„ APIï¼Œä½†æ²¡æœ‰åˆ é™¤çŸ¥è¯†åº“çš„ APIã€‚ç”¨æˆ·åˆ›å»ºé”™è¯¯çš„çŸ¥è¯†åº“åæ— æ³•é€šè¿‡ç•Œé¢åˆ é™¤ã€‚

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
# backend/app/api/knowledge.py - æ·»åŠ åˆ é™¤æ¥å£

@router.delete("/{kb_id}", status_code=204)
async def delete_knowledge_base(kb_id: str) -> None:
    """
    åˆ é™¤çŸ¥è¯†åº“åŠå…¶æ‰€æœ‰æ–‡æ¡£å’Œå‘é‡æ•°æ®

    - **kb_id**: çŸ¥è¯†åº“ ID
    """
    # 1. æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
    metadata = load_kb_metadata()
    if kb_id not in metadata:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Knowledge base '{kb_id}' not found"
        )

    # 2. åˆ é™¤ ChromaDB collection
    chroma_client = get_chroma_client()
    chroma_client.delete_collection(kb_id)

    # 3. åˆ é™¤ä¸Šä¼ çš„æ–‡ä»¶
    project_root = Path(__file__).parent.parent.parent
    upload_dir = project_root / "data" / "uploads" / kb_id
    if upload_dir.exists():
        import shutil
        shutil.rmtree(upload_dir)

    # 4. åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
    metadata_dir = project_root / "data" / "metadata" / kb_id
    if metadata_dir.exists():
        import shutil
        shutil.rmtree(metadata_dir)

    # 5. ä»çŸ¥è¯†åº“ç´¢å¼•ä¸­åˆ é™¤
    del metadata[kb_id]
    save_kb_metadata(metadata)
```

---

### 2.4 ç›¸ä¼¼åº¦åˆ†æ•°è®¡ç®—ä¸å‡†ç¡®

**é—®é¢˜æè¿°**

å½“å‰çš„ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼åœ¨æŸäº›æƒ…å†µä¸‹ä¼šäº§ç”Ÿè´Ÿæ•°æˆ–ä¸å‡†ç¡®çš„ç»“æœã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```python
# backend/app/core/rag.py:184-188
distance = results["distances"][0][i] if results["distances"] else 0
similarity = max(0, 1 - distance)  # L2 è·ç¦»èŒƒå›´å¯èƒ½è¶…è¿‡ 1
```

**é—®é¢˜åˆ†æ**

- ChromaDB é»˜è®¤ä½¿ç”¨ L2ï¼ˆæ¬§å‡ é‡Œå¾—ï¼‰è·ç¦»
- å¯¹äºå½’ä¸€åŒ–å‘é‡ï¼ŒL2 è·ç¦»èŒƒå›´æ˜¯ [0, 2]
- BGE-M3 è¾“å‡ºçš„å‘é‡é€šå¸¸å·²å½’ä¸€åŒ–
- å½“è·ç¦» > 1 æ—¶ï¼Œ`1 - distance` ä¸ºè´Ÿæ•°ï¼Œè¢«æˆªæ–­ä¸º 0

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
# æ–¹æ¡ˆ 1: ä½¿ç”¨æ›´åˆç†çš„è½¬æ¢å…¬å¼
def distance_to_similarity(distance: float) -> float:
    """å°† L2 è·ç¦»è½¬æ¢ä¸º 0-1 ç›¸ä¼¼åº¦åˆ†æ•°"""
    # ä½¿ç”¨æŒ‡æ•°è¡°å‡ï¼šè·ç¦»è¶Šå¤§ï¼Œç›¸ä¼¼åº¦è¡°å‡è¶Šå¿«
    return 1 / (1 + distance)

# æ–¹æ¡ˆ 2: æ”¹ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
# åœ¨åˆ›å»º collection æ—¶æŒ‡å®šè·ç¦»å‡½æ•°
collection = self._client.get_or_create_collection(
    name=collection_name,
    metadata={"hnsw:space": "cosine"}  # ä½¿ç”¨ä½™å¼¦è·ç¦»
)

# ä½™å¼¦è·ç¦»èŒƒå›´ [0, 2]ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ [0, 1]
def cosine_distance_to_similarity(distance: float) -> float:
    return 1 - distance / 2

# æ–¹æ¡ˆ 3: ç›´æ¥ä½¿ç”¨ ChromaDB çš„å†…ç§¯ï¼ˆéœ€è¦å½’ä¸€åŒ–å‘é‡ï¼‰
collection = self._client.get_or_create_collection(
    name=collection_name,
    metadata={"hnsw:space": "ip"}  # å†…ç§¯ï¼ˆå¯¹å½’ä¸€åŒ–å‘é‡ç­‰äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
)
# å†…ç§¯ç»“æœç›´æ¥å°±æ˜¯ç›¸ä¼¼åº¦
```

---

### 2.5 æ–‡ä»¶è¦†ç›–é£é™©

**é—®é¢˜æè¿°**

ä½¿ç”¨åŸå§‹æ–‡ä»¶åå­˜å‚¨ä¸Šä¼ æ–‡ä»¶ï¼ŒåŒåæ–‡ä»¶ä¼šç›¸äº’è¦†ç›–ã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```python
# backend/app/api/knowledge.py:124
file_path = get_upload_path(kb_id, file.filename)
with open(file_path, "wb") as f:
    f.write(content)
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
import uuid
from pathlib import Path

def get_safe_upload_path(kb_id: str, original_filename: str) -> tuple[Path, str]:
    """
    ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å­˜å‚¨è·¯å¾„

    Returns:
        (file_path, stored_filename)
    """
    project_root = Path(__file__).parent.parent.parent
    upload_dir = project_root / "data" / "uploads" / kb_id
    upload_dir.mkdir(parents=True, exist_ok=True)

    # æå–æ‰©å±•å
    suffix = Path(original_filename).suffix.lower()

    # ä½¿ç”¨ UUID ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
    unique_filename = f"{uuid.uuid4().hex}{suffix}"

    return upload_dir / unique_filename, unique_filename


# ä¿®æ”¹ä¸Šä¼ é€»è¾‘
@router.post("/{kb_id}/upload")
async def upload_document(...):
    # ...
    file_path, stored_filename = get_safe_upload_path(kb_id, file.filename)

    with open(file_path, "wb") as f:
        f.write(content)

    metadata = {
        "id": doc_id,
        "kb_id": kb_id,
        "original_filename": file.filename,  # ä¿ç•™åŸå§‹æ–‡ä»¶åç”¨äºæ˜¾ç¤º
        "stored_filename": stored_filename,   # å®é™…å­˜å‚¨çš„æ–‡ä»¶å
        "file_path": str(file_path),
        # ...
    }
```

---

## ä¸‰ã€å¹¶å‘ä¸æ€§èƒ½é—®é¢˜ï¼ˆP2ï¼‰

### 3.1 JSON æ–‡ä»¶æ— å¹¶å‘ä¿æŠ¤

**é—®é¢˜æè¿°**

æ‰€æœ‰ JSON æ–‡ä»¶æ“ä½œéƒ½æ˜¯ load â†’ ä¿®æ”¹ â†’ save æ¨¡å¼ï¼Œæ²¡æœ‰ä»»ä½•é”æœºåˆ¶ã€‚

**å—å½±å“çš„æ–‡ä»¶**

- `data/workflows.json`
- `data/kb_metadata.json`
- `data/metadata/{kb_id}/documents.json`
- `data/sessions/{session_id}.json`

**å¹¶å‘åœºæ™¯ç¤ºä¾‹**

```
æ—¶é—´çº¿:
T1: ç”¨æˆ·A load() -> {"w1": {...}}
T2: ç”¨æˆ·B load() -> {"w1": {...}}
T3: ç”¨æˆ·A æ·»åŠ  w2, save() -> {"w1": {...}, "w2": {...}}
T4: ç”¨æˆ·B æ·»åŠ  w3, save() -> {"w1": {...}, "w3": {...}}

ç»“æœ: w2 è¢«è¦†ç›–ä¸¢å¤±
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

æ–¹æ¡ˆ A: ä½¿ç”¨æ–‡ä»¶é”

```python
# å®‰è£…: pip install filelock
from filelock import FileLock

def load_workflows() -> dict:
    ensure_data_dir()
    lock = FileLock(str(WORKFLOW_FILE) + ".lock")

    with lock:
        if not WORKFLOW_FILE.exists():
            return {"workflows": {}}
        try:
            with open(WORKFLOW_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {"workflows": {}}


def save_workflows(data: dict) -> None:
    ensure_data_dir()
    lock = FileLock(str(WORKFLOW_FILE) + ".lock")

    with lock:
        with open(WORKFLOW_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
```

æ–¹æ¡ˆ B: æ”¹ç”¨ SQLiteï¼ˆæ¨èé•¿æœŸæ–¹æ¡ˆï¼‰

```python
# ä½¿ç”¨ SQLite æ›¿ä»£ JSON æ–‡ä»¶
import sqlite3
from contextlib import contextmanager

DATABASE_PATH = DATA_DIR / "agent_flow.db"

@contextmanager
def get_db():
    conn = sqlite3.connect(DATABASE_PATH)
    conn.row_factory = sqlite3.Row
    try:
        yield conn
        conn.commit()
    except Exception:
        conn.rollback()
        raise
    finally:
        conn.close()

def init_db():
    with get_db() as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS workflows (
                id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                description TEXT,
                graph_data TEXT,
                created_at TEXT,
                updated_at TEXT
            )
        """)
```

---

### 3.2 å…¨å±€å•ä¾‹åœ¨å¤šçº¿ç¨‹ä¸‹ä¸å®‰å…¨

**é—®é¢˜æè¿°**

å…¨å±€å•ä¾‹çš„åˆ›å»ºæ²¡æœ‰çº¿ç¨‹é”ä¿æŠ¤ï¼Œåœ¨å¹¶å‘åœºæ™¯ä¸‹å¯èƒ½åˆ›å»ºå¤šä¸ªå®ä¾‹ã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```python
# backend/app/core/rag.py:200-208
_rag_pipeline: Optional[RAGPipeline] = None

def get_rag_pipeline() -> RAGPipeline:
    global _rag_pipeline
    if _rag_pipeline is None:  # æ£€æŸ¥å’Œèµ‹å€¼ä¹‹é—´å­˜åœ¨ç«æ€æ¡ä»¶
        _rag_pipeline = RAGPipeline()
    return _rag_pipeline
```

**åŒæ ·é—®é¢˜å­˜åœ¨äº**

- `backend/app/core/chroma_client.py:164-174`
- `backend/app/core/config.py`ï¼ˆå¦‚æœæœ‰ç±»ä¼¼å®ç°ï¼‰

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
import threading

_rag_pipeline: Optional[RAGPipeline] = None
_rag_pipeline_lock = threading.Lock()

def get_rag_pipeline() -> RAGPipeline:
    global _rag_pipeline

    if _rag_pipeline is None:
        with _rag_pipeline_lock:
            # Double-checked locking
            if _rag_pipeline is None:
                _rag_pipeline = RAGPipeline()

    return _rag_pipeline
```

æˆ–è€…ä½¿ç”¨ FastAPI çš„ä¾èµ–æ³¨å…¥ï¼š

```python
from functools import lru_cache

@lru_cache()
def get_rag_pipeline() -> RAGPipeline:
    return RAGPipeline()
```

---

### 3.3 åå°ä»»åŠ¡æ— çŠ¶æ€è¿½è¸ª

**é—®é¢˜æè¿°**

æ–‡æ¡£å¤„ç†ä½¿ç”¨ `BackgroundTasks`ï¼Œä½†ï¼š
- æ²¡æœ‰è¿”å›ä»»åŠ¡ ID ç»™å‰ç«¯
- æ²¡æœ‰è¿›åº¦æŸ¥è¯¢æ¥å£
- å¤„ç†å¤±è´¥åç”¨æˆ·åªèƒ½åˆ·æ–°åˆ—è¡¨æŸ¥çœ‹çŠ¶æ€

**ä¼˜åŒ–æ–¹æ¡ˆ**

æ–¹æ¡ˆ A: æ·»åŠ ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æ¥å£

```python
# ä½¿ç”¨å…¨å±€å­—å…¸è¿½è¸ªä»»åŠ¡çŠ¶æ€ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redisï¼‰
processing_tasks: Dict[str, dict] = {}

@router.post("/{kb_id}/upload")
async def upload_document(...):
    # ...
    task_id = str(uuid.uuid4())

    processing_tasks[task_id] = {
        "status": "pending",
        "progress": 0,
        "doc_id": doc_id,
        "kb_id": kb_id,
        "started_at": datetime.utcnow().isoformat()
    }

    background_tasks.add_task(
        process_document_task,
        kb_id,
        doc_id,
        task_id  # ä¼ é€’ä»»åŠ¡ ID
    )

    return DocumentResponse(
        id=doc_id,
        task_id=task_id,  # è¿”å›ä»»åŠ¡ ID
        # ...
    )


@router.get("/tasks/{task_id}")
async def get_task_status(task_id: str) -> dict:
    """æŸ¥è¯¢ä»»åŠ¡å¤„ç†çŠ¶æ€"""
    if task_id not in processing_tasks:
        raise HTTPException(status_code=404, detail="Task not found")
    return processing_tasks[task_id]


def process_document_task(kb_id: str, doc_id: str, task_id: str):
    """å¸¦çŠ¶æ€æ›´æ–°çš„åå°ä»»åŠ¡"""
    try:
        processing_tasks[task_id]["status"] = "processing"
        processing_tasks[task_id]["progress"] = 10

        # ... å¤„ç†æ­¥éª¤ï¼Œæ¯æ­¥æ›´æ–°è¿›åº¦ ...

        processing_tasks[task_id]["status"] = "completed"
        processing_tasks[task_id]["progress"] = 100
    except Exception as e:
        processing_tasks[task_id]["status"] = "failed"
        processing_tasks[task_id]["error"] = str(e)
```

æ–¹æ¡ˆ B: ä½¿ç”¨ WebSocket æ¨é€è¿›åº¦

```python
# ä½¿ç”¨ FastAPI WebSocket
from fastapi import WebSocket

@router.websocket("/ws/tasks/{task_id}")
async def task_progress_ws(websocket: WebSocket, task_id: str):
    await websocket.accept()

    while True:
        if task_id in processing_tasks:
            status = processing_tasks[task_id]
            await websocket.send_json(status)

            if status["status"] in ("completed", "failed"):
                break

        await asyncio.sleep(0.5)

    await websocket.close()
```

---

## å››ã€å®‰å…¨é—®é¢˜

### 4.1 è·¯å¾„éå†æ¼æ´

**é—®é¢˜æè¿°**

æ–‡ä»¶ä¸Šä¼ ä½¿ç”¨ç”¨æˆ·æä¾›çš„æ–‡ä»¶åï¼Œå¯èƒ½å¯¼è‡´è·¯å¾„éå†æ”»å‡»ã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```python
# backend/app/api/knowledge.py:58-63
def get_upload_path(kb_id: str, filename: str) -> Path:
    upload_dir = project_root / "data" / "uploads" / kb_id
    upload_dir.mkdir(parents=True, exist_ok=True)
    return upload_dir / filename  # filename å¯èƒ½æ˜¯ "../../../etc/passwd"
```

**æ”»å‡»ç¤ºä¾‹**

```
POST /api/v1/knowledge/kb1/upload
Content-Disposition: form-data; name="file"; filename="../../../app/main.py"

# æ¶æ„å†…å®¹ä¼šè¦†ç›– main.py
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

```python
from pathlib import Path
import re

def secure_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤è·¯å¾„éå†å­—ç¬¦
    """
    # åªä¿ç•™æ–‡ä»¶åéƒ¨åˆ†
    filename = Path(filename).name

    # ç§»é™¤å±é™©å­—ç¬¦
    filename = re.sub(r'[^\w\s\-\.]', '', filename)

    # ç§»é™¤å‰å¯¼ç‚¹ï¼ˆéšè—æ–‡ä»¶ï¼‰
    filename = filename.lstrip('.')

    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å
    if not filename:
        filename = "unnamed_file"

    return filename


def get_upload_path(kb_id: str, filename: str) -> Path:
    # æ¸…ç† kb_idï¼ˆä¹Ÿå¯èƒ½è¢«æ³¨å…¥ï¼‰
    safe_kb_id = re.sub(r'[^\w\-]', '', kb_id)

    project_root = Path(__file__).parent.parent.parent
    upload_dir = project_root / "data" / "uploads" / safe_kb_id
    upload_dir.mkdir(parents=True, exist_ok=True)

    safe_filename = secure_filename(filename)
    full_path = upload_dir / safe_filename

    # æœ€åéªŒè¯ï¼šç¡®ä¿è·¯å¾„åœ¨é¢„æœŸç›®å½•å†…
    try:
        full_path.resolve().relative_to(upload_dir.resolve())
    except ValueError:
        raise ValueError("Invalid file path")

    return full_path
```

---

### 4.2 Session ID å¯é¢„æµ‹

**é—®é¢˜æè¿°**

å‰ç«¯ä½¿ç”¨ `Math.random()` ç”Ÿæˆ Session IDï¼Œä¸å¤Ÿå®‰å…¨ã€‚

**é—®é¢˜ä»£ç ä½ç½®**

```typescript
// frontend/src/views/ChatTerminal.vue:117-119
function generateId(): string {
  return Date.now().toString(36) + Math.random().toString(36).substr(2)
}
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

```typescript
function generateSecureId(): string {
  // ä½¿ç”¨ Web Crypto APIï¼ˆæµè§ˆå™¨åŸç”Ÿæ”¯æŒï¼‰
  const array = new Uint8Array(16)
  crypto.getRandomValues(array)
  return Array.from(array, byte => byte.toString(16).padStart(2, '0')).join('')
}
```

æˆ–è€…è®©åç«¯ç”Ÿæˆ Session IDï¼š

```python
# åç«¯ç”Ÿæˆå®‰å…¨çš„ Session ID
import secrets

@router.post("/sessions")
async def create_session() -> dict:
    session_id = secrets.token_urlsafe(32)
    # ... åˆ›å»ºä¼šè¯
    return {"session_id": session_id}
```

---

### 4.3 æ¡ä»¶èŠ‚ç‚¹ eval é£é™©é¢„è­¦

**é—®é¢˜æè¿°**

å¦‚æœå°†æ¥å®ç°æ¡ä»¶è¡¨è¾¾å¼æ‰§è¡Œï¼Œä½¿ç”¨ `eval()` æˆ– `exec()` ä¼šå¯¼è‡´è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰æ¼æ´ã€‚

**å±é™©ç¤ºä¾‹**

```python
# åƒä¸‡ä¸è¦è¿™æ ·åšï¼
def evaluate_condition(expression: str, context: dict) -> bool:
    return eval(expression, {"__builtins__": {}}, context)  # ä»ç„¶ä¸å®‰å…¨ï¼

# æ”»å‡»è€…è¾“å…¥: ().__class__.__bases__[0].__subclasses__()[104]().load_module('os').system('rm -rf /')
```

**å®‰å…¨æ–¹æ¡ˆ**

å‚è€ƒ 1.3 èŠ‚çš„ç»“æ„åŒ–æ¡ä»¶æ–¹æ¡ˆæˆ–ä½¿ç”¨ `simpleeval` åº“ã€‚

---

## äº”ã€æ¶æ„è®¾è®¡é—®é¢˜

### 5.1 å‰ç«¯çŠ¶æ€åˆ†æ•£

**é—®é¢˜æè¿°**

æ¯ä¸ª View ç»„ä»¶è‡ªå·±ç®¡ç†çŠ¶æ€ï¼Œæ²¡æœ‰ä½¿ç”¨ Pinia åšå…¨å±€çŠ¶æ€ç®¡ç†ã€‚

**å½“å‰çŠ¶æ€åˆ†å¸ƒ**

| ç»„ä»¶ | çŠ¶æ€ | é—®é¢˜ |
|------|------|------|
| `WorkflowEditor.vue` | workflows åˆ—è¡¨ | åˆ‡æ¢é¡µé¢åä¸¢å¤± |
| `KnowledgeView.vue` | çŸ¥è¯†åº“åˆ—è¡¨ | åˆ‡æ¢é¡µé¢åä¸¢å¤± |
| `ChatTerminal.vue` | sessions åˆ—è¡¨ | åˆ·æ–°åä¸¢å¤± |

**ä¼˜åŒ–æ–¹æ¡ˆ**

```typescript
// stores/workflow.ts
import { defineStore } from 'pinia'

export const useWorkflowStore = defineStore('workflow', {
  state: () => ({
    workflows: [] as Workflow[],
    currentWorkflowId: null as string | null,
    isLoading: false,
  }),

  getters: {
    currentWorkflow: (state) =>
      state.workflows.find(w => w.id === state.currentWorkflowId),
  },

  actions: {
    async fetchWorkflows() {
      this.isLoading = true
      try {
        const response = await axios.get('/api/v1/workflows')
        this.workflows = response.data.items
      } finally {
        this.isLoading = false
      }
    },

    async saveWorkflow(workflow: WorkflowCreate) {
      const response = await axios.post('/api/v1/workflows', workflow)
      this.workflows.unshift(response.data)
      return response.data
    },
  },
})
```

```typescript
// stores/knowledge.ts
export const useKnowledgeStore = defineStore('knowledge', {
  state: () => ({
    knowledgeBases: [] as KnowledgeBase[],
    documents: {} as Record<string, Document[]>,  // kb_id -> documents
  }),
  // ...
})
```

```typescript
// stores/chat.ts
export const useChatStore = defineStore('chat', {
  state: () => ({
    sessions: [] as Session[],
    currentSessionId: null as string | null,
  }),

  persist: true,  // ä½¿ç”¨ pinia-plugin-persistedstate æŒä¹…åŒ–
  // ...
})
```

---

### 5.2 API è¿”å›æ ¼å¼ä¸ç»Ÿä¸€

**é—®é¢˜æè¿°**

ä¸åŒæ¥å£è¿”å›æ ¼å¼ä¸ä¸€è‡´ï¼Œå‰ç«¯å¤„ç†æ—¶éœ€è¦é€‚é…å¤šç§æƒ…å†µã€‚

**å½“å‰æƒ…å†µ**

```python
# è¿”å› Pydantic æ¨¡å‹
async def create_workflow(...) -> Workflow:

# è¿”å›è£¸ dict
async def get_knowledge_base_info(...) -> dict:

# è¿”å› JSONResponse
async def delete_document(...) -> JSONResponse:
```

**ä¼˜åŒ–æ–¹æ¡ˆ**

å®šä¹‰ç»Ÿä¸€çš„å“åº”æ ¼å¼ï¼š

```python
# backend/app/models/response.py
from typing import Generic, TypeVar, Optional
from pydantic import BaseModel

T = TypeVar('T')

class APIResponse(BaseModel, Generic[T]):
    """ç»Ÿä¸€ API å“åº”æ ¼å¼"""
    success: bool = True
    data: Optional[T] = None
    message: Optional[str] = None
    error_code: Optional[str] = None


class PaginatedResponse(BaseModel, Generic[T]):
    """åˆ†é¡µå“åº”æ ¼å¼"""
    items: List[T]
    total: int
    page: int = 1
    page_size: int = 20


# ä½¿ç”¨ç¤ºä¾‹
@router.get("/{kb_id}/info", response_model=APIResponse[KnowledgeBaseInfo])
async def get_knowledge_base_info(kb_id: str) -> APIResponse[KnowledgeBaseInfo]:
    info = ...
    return APIResponse(data=info)


@router.delete("/{kb_id}/documents/{doc_id}", response_model=APIResponse[None])
async def delete_document(kb_id: str, doc_id: str) -> APIResponse[None]:
    # ... åˆ é™¤é€»è¾‘
    return APIResponse(message="Document deleted successfully")
```

---

## å…­ã€è¯¦ç»†ä¼˜åŒ–æ–¹æ¡ˆ

### 6.1 å·¥ä½œæµæ‰§è¡Œå¼•æ“å®Œæ•´å®ç°

```python
# backend/app/core/workflow_engine.py
"""
å·¥ä½œæµæ‰§è¡Œå¼•æ“

æ”¯æŒ:
- æ‹“æ‰‘æ’åºæ‰§è¡Œ
- å˜é‡ä¼ é€’
- æ¡ä»¶åˆ†æ”¯
- å¼‚æ­¥æµå¼è¾“å‡º
"""

from typing import Any, Dict, List, Optional, AsyncGenerator
from collections import deque
import asyncio

from app.core.llm import chat_completion_stream
from app.core.rag import get_rag_pipeline
from app.models.workflow import Workflow


class ExecutionContext:
    """æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œå­˜å‚¨å˜é‡å’Œä¸­é—´ç»“æœ"""

    def __init__(self, initial_input: str):
        self.variables: Dict[str, Any] = {
            "input": initial_input,
        }
        self.step_outputs: Dict[str, Any] = {}

    def set_output(self, node_id: str, value: Any):
        self.step_outputs[node_id] = value
        self.variables[f"{node_id}.output"] = value

    def get_variable(self, var_path: str) -> Any:
        """è§£æå˜é‡è·¯å¾„ï¼Œå¦‚ 'step1.output'"""
        parts = var_path.split('.')
        current = self.variables
        for part in parts:
            if isinstance(current, dict):
                current = current.get(part)
            else:
                return None
        return current

    def resolve_template(self, template: str) -> str:
        """è§£ææ¨¡æ¿å­—ç¬¦ä¸²ä¸­çš„å˜é‡å¼•ç”¨ {{var}}"""
        import re

        def replace_var(match):
            var_path = match.group(1)
            value = self.get_variable(var_path)
            return str(value) if value is not None else match.group(0)

        return re.sub(r'\{\{(\w+(?:\.\w+)*)\}\}', replace_var, template)


class WorkflowEngine:
    """å·¥ä½œæµæ‰§è¡Œå¼•æ“"""

    def __init__(self, workflow: Workflow):
        self.workflow = workflow
        self.nodes = {n["id"]: n for n in workflow.graph_data.nodes}
        self.edges = workflow.graph_data.edges
        self.adjacency = self._build_adjacency()

    def _build_adjacency(self) -> Dict[str, List[str]]:
        """æ„å»ºé‚»æ¥è¡¨"""
        adj: Dict[str, List[str]] = {node_id: [] for node_id in self.nodes}
        for edge in self.edges:
            source = edge["source"]
            target = edge["target"]
            if source in adj:
                adj[source].append(target)
        return adj

    def _topological_sort(self) -> List[str]:
        """æ‹“æ‰‘æ’åº"""
        in_degree = {node_id: 0 for node_id in self.nodes}

        for edge in self.edges:
            target = edge["target"]
            if target in in_degree:
                in_degree[target] += 1

        queue = deque([node_id for node_id, degree in in_degree.items() if degree == 0])
        result = []

        while queue:
            node_id = queue.popleft()
            result.append(node_id)

            for neighbor in self.adjacency.get(node_id, []):
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        if len(result) != len(self.nodes):
            raise ValueError("Workflow contains cycles")

        return result

    def _find_start_node(self) -> Optional[str]:
        """æ‰¾åˆ°å¼€å§‹èŠ‚ç‚¹"""
        for node_id, node in self.nodes.items():
            if node.get("type") == "start":
                return node_id
        return None

    async def _execute_start_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """æ‰§è¡Œå¼€å§‹èŠ‚ç‚¹"""
        yield {
            "type": "node_start",
            "node_id": node["id"],
            "node_type": "start"
        }

        # å¼€å§‹èŠ‚ç‚¹ç›´æ¥ä¼ é€’è¾“å…¥
        output = ctx.variables["input"]
        ctx.set_output(node["id"], output)

        yield {
            "type": "node_complete",
            "node_id": node["id"],
            "output": output
        }

    async def _execute_llm_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """æ‰§è¡Œ LLM èŠ‚ç‚¹"""
        yield {
            "type": "node_start",
            "node_id": node["id"],
            "node_type": "llm"
        }

        data = node.get("data", {})
        system_prompt = data.get("systemPrompt", "You are a helpful assistant.")
        temperature = data.get("temperature", 0.7)

        # è§£æç³»ç»Ÿæç¤ºè¯ä¸­çš„å˜é‡
        system_prompt = ctx.resolve_template(system_prompt)

        # è·å–è¾“å…¥ï¼ˆæ¥è‡ªä¸Šä¸€ä¸ªèŠ‚ç‚¹ï¼‰
        input_text = ctx.variables.get("input", "")
        for source_id in self._get_source_nodes(node["id"]):
            if source_id in ctx.step_outputs:
                input_text = ctx.step_outputs[source_id]
                break

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": str(input_text)}
        ]

        output = ""
        async for token in chat_completion_stream(messages, temperature=temperature):
            output += token
            yield {
                "type": "token",
                "node_id": node["id"],
                "content": token
            }

        ctx.set_output(node["id"], output)

        yield {
            "type": "node_complete",
            "node_id": node["id"],
            "output": output[:100] + "..." if len(output) > 100 else output
        }

    async def _execute_knowledge_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """æ‰§è¡ŒçŸ¥è¯†åº“æ£€ç´¢èŠ‚ç‚¹"""
        yield {
            "type": "node_start",
            "node_id": node["id"],
            "node_type": "knowledge"
        }

        data = node.get("data", {})
        kb_id = data.get("knowledgeBaseId")

        if not kb_id:
            yield {
                "type": "node_error",
                "node_id": node["id"],
                "error": "Knowledge base not configured"
            }
            return

        # è·å–æŸ¥è¯¢æ–‡æœ¬
        query = ctx.variables.get("input", "")
        for source_id in self._get_source_nodes(node["id"]):
            if source_id in ctx.step_outputs:
                query = str(ctx.step_outputs[source_id])
                break

        yield {
            "type": "thought",
            "node_id": node["id"],
            "content": f"Searching knowledge base: {kb_id}"
        }

        try:
            rag_pipeline = get_rag_pipeline()
            results = rag_pipeline.search(kb_id, query, top_k=5)

            # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
            context_parts = []
            for i, r in enumerate(results[:3], 1):
                context_parts.append(f"[{i}] {r['text']}")

            output = "\n\n".join(context_parts) if context_parts else "No relevant documents found."
            ctx.set_output(node["id"], output)

            yield {
                "type": "node_complete",
                "node_id": node["id"],
                "output": f"Found {len(results)} relevant chunks",
                "results": results[:3]
            }

        except Exception as e:
            yield {
                "type": "node_error",
                "node_id": node["id"],
                "error": str(e)
            }

    async def _execute_condition_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """æ‰§è¡Œæ¡ä»¶èŠ‚ç‚¹"""
        yield {
            "type": "node_start",
            "node_id": node["id"],
            "node_type": "condition"
        }

        data = node.get("data", {})
        expression = data.get("expression", "true")

        # è§£æè¡¨è¾¾å¼ä¸­çš„å˜é‡
        resolved_expr = ctx.resolve_template(expression)

        # ä½¿ç”¨å®‰å…¨çš„è¡¨è¾¾å¼æ±‚å€¼
        try:
            from simpleeval import simple_eval
            result = simple_eval(resolved_expr)
        except Exception as e:
            # å¦‚æœ simpleeval æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•çš„æ¯”è¾ƒ
            result = resolved_expr.lower() in ("true", "yes", "1")

        ctx.set_output(node["id"], result)

        yield {
            "type": "node_complete",
            "node_id": node["id"],
            "output": result,
            "branch": "true" if result else "false"
        }

    async def _execute_end_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """æ‰§è¡Œç»“æŸèŠ‚ç‚¹"""
        yield {
            "type": "node_start",
            "node_id": node["id"],
            "node_type": "end"
        }

        # æ”¶é›†æ‰€æœ‰ä¸Šæ¸¸èŠ‚ç‚¹çš„è¾“å‡º
        final_output = None
        for source_id in self._get_source_nodes(node["id"]):
            if source_id in ctx.step_outputs:
                final_output = ctx.step_outputs[source_id]
                break

        ctx.set_output(node["id"], final_output)

        yield {
            "type": "node_complete",
            "node_id": node["id"],
            "output": final_output
        }

        yield {
            "type": "workflow_complete",
            "final_output": final_output
        }

    def _get_source_nodes(self, node_id: str) -> List[str]:
        """è·å–æŒ‡å‘è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰æºèŠ‚ç‚¹"""
        sources = []
        for edge in self.edges:
            if edge["target"] == node_id:
                sources.append(edge["source"])
        return sources

    async def _execute_node(
        self,
        node_id: str,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        node = self.nodes[node_id]
        node_type = node.get("type", "unknown")

        executors = {
            "start": self._execute_start_node,
            "llm": self._execute_llm_node,
            "knowledge": self._execute_knowledge_node,
            "condition": self._execute_condition_node,
            "end": self._execute_end_node,
        }

        executor = executors.get(node_type)
        if executor:
            async for event in executor(node, ctx):
                yield event
        else:
            yield {
                "type": "node_error",
                "node_id": node_id,
                "error": f"Unknown node type: {node_type}"
            }

    async def execute(
        self,
        initial_input: str
    ) -> AsyncGenerator[dict, None]:
        """
        æ‰§è¡Œæ•´ä¸ªå·¥ä½œæµ

        Yields:
            æ‰§è¡Œäº‹ä»¶æµï¼ŒåŒ…æ‹¬:
            - workflow_start
            - node_start
            - token (LLM è¾“å‡º)
            - thought (æ€è€ƒè¿‡ç¨‹)
            - node_complete
            - node_error
            - workflow_complete
        """
        yield {
            "type": "workflow_start",
            "workflow_id": self.workflow.id,
            "workflow_name": self.workflow.name
        }

        try:
            execution_order = self._topological_sort()
            ctx = ExecutionContext(initial_input)

            for node_id in execution_order:
                async for event in self._execute_node(node_id, ctx):
                    yield event

                    # å¦‚æœé‡åˆ°é”™è¯¯ï¼Œåœæ­¢æ‰§è¡Œ
                    if event.get("type") == "node_error":
                        yield {
                            "type": "workflow_error",
                            "error": event.get("error")
                        }
                        return

        except Exception as e:
            yield {
                "type": "workflow_error",
                "error": str(e)
            }
```

---

### 6.2 ä¿®æ”¹ Chat API æ”¯æŒå·¥ä½œæµæ‰§è¡Œ

```python
# backend/app/api/chat.py - ä¿®æ”¹ç‰ˆ

from app.core.workflow_engine import WorkflowEngine
from app.api.workflow import get_workflow

@router.post("/completions")
async def chat_completions(request: ChatRequest) -> StreamingResponse:
    if not request.message.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Message cannot be empty"
        )

    # åŠ è½½æˆ–åˆ›å»ºä¼šè¯
    session = load_session(request.session_id)
    if session is None:
        session = SessionHistory(
            session_id=request.session_id,
            kb_id=request.kb_id,
            workflow_id=request.workflow_id
        )

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    user_message = ChatMessage(
        role="user",
        content=request.message,
        timestamp=datetime.utcnow()
    )
    session.messages.append(user_message)

    # åˆ¤æ–­æ‰§è¡Œæ¨¡å¼
    if request.workflow_id:
        # å·¥ä½œæµæ¨¡å¼
        return StreamingResponse(
            workflow_stream(request, session),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )
    else:
        # æ™®é€š RAG å¯¹è¯æ¨¡å¼ï¼ˆç°æœ‰é€»è¾‘ï¼‰
        return StreamingResponse(
            rag_stream(request, session),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
            }
        )


async def workflow_stream(
    request: ChatRequest,
    session: SessionHistory
) -> AsyncGenerator[str, None]:
    """å·¥ä½œæµæ‰§è¡Œæµ"""

    # åŠ è½½å·¥ä½œæµ
    try:
        workflow = await get_workflow(request.workflow_id)
    except HTTPException:
        yield format_sse_event("error", {
            "message": f"Workflow not found: {request.workflow_id}"
        })
        return

    # åˆ›å»ºæ‰§è¡Œå¼•æ“
    engine = WorkflowEngine(workflow)

    # æ”¶é›†å®Œæ•´è¾“å‡º
    full_output = ""

    # æ‰§è¡Œå·¥ä½œæµå¹¶æµå¼è¾“å‡º
    async for event in engine.execute(request.message):
        event_type = event.get("type", "unknown")

        if event_type == "workflow_start":
            yield format_sse_event("thought", {
                "type": "workflow",
                "status": "start",
                "workflow_name": event.get("workflow_name")
            })

        elif event_type == "node_start":
            yield format_sse_event("thought", {
                "type": "node",
                "status": "start",
                "node_id": event.get("node_id"),
                "node_type": event.get("node_type")
            })

        elif event_type == "token":
            full_output += event.get("content", "")
            yield format_sse_event("token", {
                "content": event.get("content")
            })

        elif event_type == "thought":
            yield format_sse_event("thought", {
                "type": "thinking",
                "content": event.get("content")
            })

        elif event_type == "node_complete":
            yield format_sse_event("thought", {
                "type": "node",
                "status": "complete",
                "node_id": event.get("node_id")
            })

        elif event_type == "workflow_complete":
            final_output = event.get("final_output", full_output)

            # ä¿å­˜åŠ©æ‰‹å›å¤
            assistant_message = ChatMessage(
                role="assistant",
                content=str(final_output),
                timestamp=datetime.utcnow()
            )
            session.messages.append(assistant_message)
            save_session(session)

            yield format_sse_event("done", {
                "status": "success",
                "message": "Workflow completed"
            })

        elif event_type in ("node_error", "workflow_error"):
            yield format_sse_event("error", {
                "message": event.get("error")
            })
            yield format_sse_event("done", {
                "status": "error",
                "message": event.get("error")
            })
```

---

## ä¸ƒã€ä¼˜å…ˆçº§æ’åºä¸å®æ–½è·¯çº¿å›¾

### ä¼˜å…ˆçº§å®šä¹‰

| çº§åˆ« | å«ä¹‰ | æ ‡å‡† |
|------|------|------|
| **P0** | é˜»æ–­æ€§ | æ ¸å¿ƒåŠŸèƒ½æ— æ³•å·¥ä½œ |
| **P1** | ä¸¥é‡ | åŠŸèƒ½ç¼ºé™·æˆ–æ•°æ®ä¸¢å¤± |
| **P2** | ä¸­ç­‰ | æ€§èƒ½é—®é¢˜æˆ–ç”¨æˆ·ä½“éªŒå·® |
| **P3** | è½»å¾® | ä»£ç è´¨é‡æˆ–æ¶æ„ä¼˜åŒ– |

### é—®é¢˜ä¼˜å…ˆçº§æ€»è§ˆ

| # | é—®é¢˜ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | å½±å“ |
|---|------|--------|--------|------|
| 1.1 | å·¥ä½œæµæ‰§è¡Œå¼•æ“ç¼ºå¤± | P0 | å¤§ | æ ¸å¿ƒåŠŸèƒ½ |
| 1.2 | Chat ä¸ Workflow å‰²è£‚ | P0 | ä¸­ | æ ¸å¿ƒåŠŸèƒ½ |
| 1.3 | æ¡ä»¶è¡¨è¾¾å¼æ— æ³•æ‰§è¡Œ | P0 | ä¸­ | æ ¸å¿ƒåŠŸèƒ½ |
| 1.4 | åˆ é™¤æ–‡æ¡£å‘é‡æ®‹ç•™ | P0 | å° | æ•°æ®ä¸€è‡´æ€§ |
| 1.5 | RAG æœç´¢é‡å¤æ‰§è¡Œ | P1 | å° | æ€§èƒ½ |
| 2.1 | ä¿å­˜å·¥ä½œæµä¸¢å¤±æ•°æ® | P0 | å° | æ•°æ®ä¸¢å¤± |
| 2.2 | ä¼šè¯å†å²ä¸åŒæ­¥ | P1 | ä¸­ | ç”¨æˆ·ä½“éªŒ |
| 2.3 | çŸ¥è¯†åº“åˆ é™¤ API | P1 | å° | åŠŸèƒ½å®Œæ•´æ€§ |
| 2.4 | ç›¸ä¼¼åº¦è®¡ç®—ä¸å‡† | P2 | å° | å‡†ç¡®æ€§ |
| 2.5 | æ–‡ä»¶è¦†ç›–é£é™© | P1 | å° | æ•°æ®å®‰å…¨ |
| 3.1 | JSON æ— å¹¶å‘ä¿æŠ¤ | P2 | ä¸­ | æ•°æ®ä¸€è‡´æ€§ |
| 3.2 | å•ä¾‹çº¿ç¨‹ä¸å®‰å…¨ | P2 | å° | ç¨³å®šæ€§ |
| 3.3 | åå°ä»»åŠ¡æ— è¿½è¸ª | P2 | ä¸­ | ç”¨æˆ·ä½“éªŒ |
| 4.1 | è·¯å¾„éå†æ¼æ´ | P0 | å° | å®‰å…¨ |
| 4.2 | Session ID å¯é¢„æµ‹ | P2 | å° | å®‰å…¨ |
| 5.1 | å‰ç«¯çŠ¶æ€åˆ†æ•£ | P3 | å¤§ | ä»£ç è´¨é‡ |
| 5.2 | API æ ¼å¼ä¸ç»Ÿä¸€ | P3 | ä¸­ | ä»£ç è´¨é‡ |

### å®æ–½è·¯çº¿å›¾

#### é˜¶æ®µ 1: ç´§æ€¥ä¿®å¤ï¼ˆ1-2 å¤©ï¼‰

1. âœ… ä¿®å¤ä¿å­˜å·¥ä½œæµä¸¢å¤± data å­—æ®µ
2. âœ… ä¿®å¤åˆ é™¤æ–‡æ¡£å‘é‡æ®‹ç•™
3. âœ… ä¿®å¤è·¯å¾„éå†æ¼æ´
4. âœ… ä¿®å¤ RAG æœç´¢é‡å¤æ‰§è¡Œ

#### é˜¶æ®µ 2: æ ¸å¿ƒåŠŸèƒ½è¡¥å…¨ï¼ˆ3-5 å¤©ï¼‰

1. ğŸ”¨ å®ç°å·¥ä½œæµæ‰§è¡Œå¼•æ“
2. ğŸ”¨ ä¿®æ”¹ Chat API æ”¯æŒå·¥ä½œæµ
3. ğŸ”¨ å®ç°æ¡ä»¶è¡¨è¾¾å¼å®‰å…¨æ±‚å€¼
4. ğŸ”¨ å‰ç«¯ Chat æ·»åŠ å·¥ä½œæµ/çŸ¥è¯†åº“é€‰æ‹©

#### é˜¶æ®µ 3: åŠŸèƒ½å®Œå–„ï¼ˆ2-3 å¤©ï¼‰

1. ğŸ“ æ·»åŠ çŸ¥è¯†åº“åˆ é™¤ API
2. ğŸ“ å®ç°ä¼šè¯å†å²åŒæ­¥
3. ğŸ“ ä¿®å¤ç›¸ä¼¼åº¦è®¡ç®—
4. ğŸ“ ä¿®å¤æ–‡ä»¶è¦†ç›–é£é™©

#### é˜¶æ®µ 4: ç¨³å®šæ€§ä¼˜åŒ–ï¼ˆ2-3 å¤©ï¼‰

1. ğŸ”§ æ·»åŠ  JSON æ–‡ä»¶å¹¶å‘é”
2. ğŸ”§ ä¿®å¤å…¨å±€å•ä¾‹çº¿ç¨‹å®‰å…¨
3. ğŸ”§ æ·»åŠ åå°ä»»åŠ¡çŠ¶æ€è¿½è¸ª
4. ğŸ”§ æ”¹è¿› Session ID ç”Ÿæˆ

#### é˜¶æ®µ 5: æ¶æ„ä¼˜åŒ–ï¼ˆå¯é€‰ï¼Œ3-5 å¤©ï¼‰

1. ğŸ— å‰ç«¯çŠ¶æ€ç®¡ç†é‡æ„ï¼ˆPiniaï¼‰
2. ğŸ— API å“åº”æ ¼å¼ç»Ÿä¸€
3. ğŸ— è€ƒè™‘ä½¿ç”¨ SQLite æ›¿ä»£ JSON

---

## æ€»ç»“

è¿™ä»½å®¡æŸ¥æŠ¥å‘Šè¯†åˆ«äº† **17 ä¸ªé—®é¢˜**ï¼Œå…¶ä¸­ï¼š

- **P0 çº§åˆ«**: 6 ä¸ªï¼ˆæ ¸å¿ƒåŠŸèƒ½é˜»æ–­ï¼‰
- **P1 çº§åˆ«**: 4 ä¸ªï¼ˆä¸¥é‡ç¼ºé™·ï¼‰
- **P2 çº§åˆ«**: 5 ä¸ªï¼ˆä¸­ç­‰é—®é¢˜ï¼‰
- **P3 çº§åˆ«**: 2 ä¸ªï¼ˆä»£ç è´¨é‡ï¼‰

**æœ€æ ¸å¿ƒçš„é—®é¢˜æ˜¯**ï¼šå·¥ä½œæµç¼–è¾‘å™¨å’Œ Chat å¯¹è¯å®Œå…¨å‰²è£‚ã€‚ä½ ç”»äº†ä¸€ä¸ªæ¼‚äº®çš„å›¾ï¼Œä½†å®ƒæ°¸è¿œä¸ä¼šè¢«æ‰§è¡Œã€‚è¦è®©è¿™ä¸ªé¡¹ç›®è¾¾åˆ°è®¾è®¡ç›®æ ‡ï¼Œå¿…é¡»å…ˆå®ç°å·¥ä½œæµæ‰§è¡Œå¼•æ“ï¼Œå¹¶å°†å…¶ä¸ Chat ç³»ç»Ÿæ‰“é€šã€‚

å»ºè®®æŒ‰ç…§è·¯çº¿å›¾åˆ†é˜¶æ®µå®æ–½ï¼Œå…ˆä¿®å¤ç´§æ€¥é—®é¢˜ï¼Œå†è¡¥å…¨æ ¸å¿ƒåŠŸèƒ½ï¼Œæœ€åä¼˜åŒ–æ¶æ„ã€‚
