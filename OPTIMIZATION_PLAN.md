# Agent Flow Lite ä¼˜åŒ–æ‰§è¡Œè®¡åˆ’

> æœ¬æ–‡æ¡£ä¸º AI æ‰§è¡Œæ‰‹å†Œï¼ŒåŒ…å«å¯ç›´æ¥æ‰§è¡Œçš„å…·ä½“æŒ‡ä»¤
> æ‰§è¡Œé¡ºåºï¼šæŒ‰ Phase é¡ºåºæ‰§è¡Œï¼Œæ¯ä¸ª Phase å†…çš„ Task å¯å¹¶è¡Œ

---

## æ‰§è¡Œæ¦‚è§ˆ

```
Phase 1: ç´§æ€¥ä¿®å¤ï¼ˆæ•°æ®å®‰å…¨ï¼‰     â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  4 tasks
Phase 2: æ ¸å¿ƒåŠŸèƒ½ï¼ˆå·¥ä½œæµå¼•æ“ï¼‰   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  6 tasks
Phase 3: åŠŸèƒ½å®Œå–„               â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘  4 tasks
Phase 4: ç¨³å®šæ€§ä¼˜åŒ–             â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  3 tasks
```

---

# Phase 1: ç´§æ€¥ä¿®å¤

## Task 1.1: ä¿®å¤ä¿å­˜å·¥ä½œæµä¸¢å¤±èŠ‚ç‚¹é…ç½®æ•°æ®

### é—®é¢˜
ä¿å­˜å·¥ä½œæµæ—¶ï¼ŒèŠ‚ç‚¹çš„ `data` å­—æ®µï¼ˆåŒ…å« systemPromptã€temperature ç­‰é…ç½®ï¼‰è¢«ä¸¢å¼ƒã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `frontend/src/views/WorkflowEditor.vue`

**æ­¥éª¤ 1**: æ‰¾åˆ° `saveWorkflow` å‡½æ•°ï¼ˆçº¦ç¬¬ 209 è¡Œï¼‰ï¼Œä¿®æ”¹ nodes æ˜ å°„é€»è¾‘ï¼š

```typescript
// ä¿®æ”¹å‰ï¼ˆçº¦ç¬¬ 222-227 è¡Œï¼‰
nodes: flowData.nodes.map((n: any) => ({
  id: n.id,
  type: n.type,
  position: n.position,
  label: n.label
})),

// ä¿®æ”¹å
nodes: flowData.nodes.map((n: any) => ({
  id: n.id,
  type: n.type,
  position: n.position,
  label: n.label,
  data: n.data || {}
})),
```

**æ­¥éª¤ 2**: æ‰¾åˆ° `loadWorkflow` å‡½æ•°ï¼ˆçº¦ç¬¬ 258 è¡Œï¼‰ï¼Œä¿®æ”¹åŠ è½½é€»è¾‘ï¼š

```typescript
// ä¿®æ”¹å‰ï¼ˆçº¦ç¬¬ 265-271 è¡Œï¼‰
if (graphData && graphData.nodes) {
  setNodes(graphData.nodes.map((n: any) => ({
    id: n.id,
    type: n.type,
    position: n.position,
    label: n.label || (n.type === 'start' ? 'å¼€å§‹' : n.type === 'llm' ? 'LLM' : 'çŸ¥è¯†åº“'),
    data: n.data || {}
  })))
}

// ä¿®æ”¹åï¼ˆç¡®ä¿ data å­—æ®µè¢«æ­£ç¡®åŠ è½½ï¼‰
if (graphData && graphData.nodes) {
  setNodes(graphData.nodes.map((n: any) => ({
    id: n.id,
    type: n.type,
    position: n.position,
    label: n.label || getDefaultLabel(n.type),
    data: n.data || {}
  })))
}

// åœ¨ script setup ä¸­æ·»åŠ è¾…åŠ©å‡½æ•°
function getDefaultLabel(type: string): string {
  const labelMap: Record<string, string> = {
    start: 'å¼€å§‹',
    llm: 'LLM',
    knowledge: 'çŸ¥è¯†åº“',
    end: 'ç»“æŸ',
    condition: 'æ¡ä»¶'
  }
  return labelMap[type] || type
}
```

### éªŒæ”¶æ ‡å‡†
1. åˆ›å»ºå·¥ä½œæµï¼Œæ·»åŠ  LLM èŠ‚ç‚¹
2. é…ç½® LLM èŠ‚ç‚¹çš„ systemPrompt å’Œ temperature
3. ä¿å­˜å·¥ä½œæµ
4. åˆ·æ–°é¡µé¢ï¼ŒåŠ è½½è¯¥å·¥ä½œæµ
5. ç‚¹å‡» LLM èŠ‚ç‚¹ï¼Œç¡®è®¤é…ç½®ä»ç„¶å­˜åœ¨

---

## Task 1.2: ä¿®å¤åˆ é™¤æ–‡æ¡£æ—¶å‘é‡æ•°æ®æ®‹ç•™

### é—®é¢˜
åˆ é™¤æ–‡æ¡£æ—¶ï¼ŒChromaDB ä¸­çš„ chunk æ•°æ®æ²¡æœ‰è¢«åˆ é™¤ï¼ˆID æ ¼å¼ä¸åŒ¹é…ï¼‰ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/core/chroma_client.py`

**ä¿®æ”¹ `delete_document` æ–¹æ³•ï¼ˆçº¦ç¬¬ 117-133 è¡Œï¼‰**ï¼š

```python
# ä¿®æ”¹å‰
def delete_document(self, kb_id: str, document_id: str) -> bool:
    """
    Delete a document from a knowledge base collection.
    """
    try:
        collection = self._client.get_collection(name=f"kb_{kb_id}")
        collection.delete(ids=[document_id])
        return True
    except ValueError:
        return False

# ä¿®æ”¹å
def delete_document(self, kb_id: str, document_id: str) -> bool:
    """
    Delete a document and all its chunks from a knowledge base collection.

    Args:
        kb_id: Knowledge base ID
        document_id: Document ID to delete (will delete all chunks with this doc_id)

    Returns:
        True if chunks were deleted, False if collection doesn't exist
    """
    try:
        collection = self._client.get_collection(name=f"kb_{kb_id}")

        # ä½¿ç”¨ metadata è¿‡æ»¤åˆ é™¤æ‰€æœ‰å±äºè¯¥æ–‡æ¡£çš„ chunks
        # chunk IDs æ ¼å¼ä¸º: {doc_id}_chunk_0, {doc_id}_chunk_1, ...
        # ä½†æˆ‘ä»¬é€šè¿‡ metadata ä¸­çš„ doc_id å­—æ®µæ¥åˆ é™¤æ›´å¯é 
        collection.delete(where={"doc_id": document_id})

        return True
    except ValueError:
        # Collection doesn't exist
        return False
    except Exception as e:
        print(f"Error deleting document {document_id}: {e}")
        return False
```

### éªŒæ”¶æ ‡å‡†
1. ä¸Šä¼ ä¸€ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“
2. ç­‰å¾…å¤„ç†å®Œæˆ
3. åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ï¼Œç¡®è®¤èƒ½æ‰¾åˆ°å†…å®¹
4. åˆ é™¤è¯¥æ–‡æ¡£
5. å†æ¬¡æœç´¢ç›¸åŒå†…å®¹ï¼Œç¡®è®¤æœç´¢ç»“æœä¸ºç©º

---

## Task 1.3: ä¿®å¤è·¯å¾„éå†å®‰å…¨æ¼æ´

### é—®é¢˜
æ–‡ä»¶ä¸Šä¼ ä½¿ç”¨åŸå§‹æ–‡ä»¶åï¼Œå¯èƒ½å¯¼è‡´è·¯å¾„éå†æ”»å‡»ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/knowledge.py`

**æ­¥éª¤ 1**: åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥å’Œè¾…åŠ©å‡½æ•°ï¼ˆçº¦ç¬¬ 10 è¡Œåï¼‰ï¼š

```python
import re
import uuid
from pathlib import Path

def secure_filename(filename: str) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œé˜²æ­¢è·¯å¾„éå†æ”»å‡»

    Args:
        filename: åŸå§‹æ–‡ä»¶å

    Returns:
        å®‰å…¨çš„æ–‡ä»¶å
    """
    # åªä¿ç•™æ–‡ä»¶åéƒ¨åˆ†ï¼ˆå»é™¤è·¯å¾„ï¼‰
    filename = Path(filename).name

    # åªä¿ç•™å®‰å…¨å­—ç¬¦ï¼šå­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ã€ç‚¹
    filename = re.sub(r'[^\w\s\-\.]', '', filename)

    # ç§»é™¤å‰å¯¼ç‚¹ï¼ˆé˜²æ­¢éšè—æ–‡ä»¶ï¼‰
    filename = filename.lstrip('.')

    # ç§»é™¤å¤šä½™ç©ºæ ¼
    filename = ' '.join(filename.split())

    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œç”Ÿæˆé»˜è®¤å
    if not filename or filename == '.':
        filename = "unnamed_file"

    return filename
```

**æ­¥éª¤ 2**: ä¿®æ”¹ `get_upload_path` å‡½æ•°ï¼ˆçº¦ç¬¬ 58-63 è¡Œï¼‰ï¼š

```python
# ä¿®æ”¹å‰
def get_upload_path(kb_id: str, filename: str) -> Path:
    """Get the full path for saving an uploaded file."""
    project_root = Path(__file__).parent.parent.parent
    upload_dir = project_root / "data" / "uploads" / kb_id
    upload_dir.mkdir(parents=True, exist_ok=True)
    return upload_dir / filename

# ä¿®æ”¹å
def get_upload_path(kb_id: str, filename: str) -> tuple[Path, str]:
    """
    Get a safe path for saving an uploaded file.

    Args:
        kb_id: Knowledge base ID
        filename: Original filename from user

    Returns:
        Tuple of (full_path, stored_filename)
    """
    # æ¸…ç† kb_idï¼ˆä¹Ÿå¯èƒ½è¢«æ³¨å…¥ï¼‰
    safe_kb_id = re.sub(r'[^\w\-]', '', kb_id)
    if not safe_kb_id:
        safe_kb_id = "default"

    project_root = Path(__file__).parent.parent.parent
    upload_dir = project_root / "data" / "uploads" / safe_kb_id
    upload_dir.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨ UUID å‰ç¼€ç¡®ä¿å”¯ä¸€æ€§ï¼Œä¿ç•™åŸå§‹æ‰©å±•å
    safe_name = secure_filename(filename)
    suffix = Path(safe_name).suffix.lower()
    unique_filename = f"{uuid.uuid4().hex[:12]}_{safe_name}"

    full_path = upload_dir / unique_filename

    # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿è·¯å¾„åœ¨é¢„æœŸç›®å½•å†…
    try:
        full_path.resolve().relative_to(upload_dir.resolve())
    except ValueError:
        raise ValueError("Invalid file path detected")

    return full_path, unique_filename
```

**æ­¥éª¤ 3**: ä¿®æ”¹ `upload_document` å‡½æ•°ä¸­çš„è°ƒç”¨ï¼ˆçº¦ç¬¬ 124-127 è¡Œï¼‰ï¼š

```python
# ä¿®æ”¹å‰
file_path = get_upload_path(kb_id, file.filename)
with open(file_path, "wb") as f:
    f.write(content)

# ä¿®æ”¹å
file_path, stored_filename = get_upload_path(kb_id, file.filename)
with open(file_path, "wb") as f:
    f.write(content)
```

**æ­¥éª¤ 4**: ä¿®æ”¹ metadata å­˜å‚¨ï¼ˆçº¦ç¬¬ 128-137 è¡Œï¼‰ï¼š

```python
# ä¿®æ”¹å‰
metadata = {
    "id": doc_id,
    "kb_id": kb_id,
    "filename": file.filename,
    "file_path": str(file_path),
    # ...
}

# ä¿®æ”¹å
metadata = {
    "id": doc_id,
    "kb_id": kb_id,
    "original_filename": file.filename,  # åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
    "stored_filename": stored_filename,   # å®é™…å­˜å‚¨çš„æ–‡ä»¶å
    "file_path": str(file_path),
    "file_size": file_size,
    "status": DocumentStatus.PENDING.value,
    "created_at": timestamp.isoformat(),
    "updated_at": None
}
```

### éªŒæ”¶æ ‡å‡†
1. å°è¯•ä¸Šä¼ æ–‡ä»¶åä¸º `../../../etc/passwd` çš„æ–‡ä»¶
2. ç¡®è®¤æ–‡ä»¶è¢«å®‰å…¨é‡å‘½åå¹¶å­˜å‚¨åœ¨æ­£ç¡®ç›®å½•
3. ç¡®è®¤æ— æ³•éå†åˆ°ä¸Šçº§ç›®å½•

---

## Task 1.4: ä¿®å¤ RAG æœç´¢é‡å¤æ‰§è¡Œ

### é—®é¢˜
æ¯æ¬¡èŠå¤©è¯·æ±‚ä¸­ï¼ŒRAG æœç´¢è¢«æ‰§è¡Œäº†ä¸¤æ¬¡ï¼ˆä¸€æ¬¡åœ¨ä¸»å‡½æ•°ï¼Œä¸€æ¬¡åœ¨ generatorï¼‰ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/chat.py`

**æ­¥éª¤ 1**: ä¿®æ”¹ `chat_stream_generator` å‡½æ•°ç­¾åï¼ˆçº¦ç¬¬ 77 è¡Œï¼‰ï¼š

```python
# ä¿®æ”¹å‰
async def chat_stream_generator(
    request: ChatRequest, messages: List[dict]
) -> AsyncGenerator[str, None]:

# ä¿®æ”¹å
async def chat_stream_generator(
    request: ChatRequest,
    messages: List[dict],
    pre_retrieved_results: Optional[List[dict]] = None
) -> AsyncGenerator[str, None]:
```

**æ­¥éª¤ 2**: ä¿®æ”¹ generator å†…çš„ RAG é€»è¾‘ï¼ˆçº¦ç¬¬ 94-154 è¡Œï¼‰ï¼š

```python
# ä¿®æ”¹å‰ï¼ˆçº¦ç¬¬ 94-114 è¡Œï¼‰
# Step 1: RAG Retrieval (if kb_id provided)
if request.kb_id:
    yield format_sse_event("thought", {
        "type": "retrieval",
        "status": "start",
        "kb_id": request.kb_id,
        "query": request.message
    })

    try:
        rag_pipeline = get_rag_pipeline()
        yield format_sse_event("thought", {
            "type": "retrieval",
            "status": "searching",
            "kb_id": request.kb_id,
            "query": request.message
        })

        retrieved_results = rag_pipeline.search(
            request.kb_id, request.message, top_k=5
        )
        # ... åç»­å¤„ç†

# ä¿®æ”¹å
# Step 1: Use pre-retrieved results or skip RAG
retrieved_results: List[dict] = pre_retrieved_results or []

if request.kb_id and pre_retrieved_results:
    # å·²æœ‰æ£€ç´¢ç»“æœï¼Œå‘é€çŠ¶æ€äº‹ä»¶
    yield format_sse_event("thought", {
        "type": "retrieval",
        "status": "complete",
        "kb_id": request.kb_id,
        "query": request.message,
        "results_count": len(retrieved_results),
        "top_results": [
            {
                "text": r["text"][:200] + "..." if len(r["text"]) > 200 else r["text"],
                "doc_id": r["metadata"].get("doc_id", ""),
                "score": r["score"]
            }
            for r in retrieved_results[:3]
        ]
    })

    # Send citations if results found
    if retrieved_results:
        sources = [
            {
                "doc_id": r["metadata"].get("doc_id", ""),
                "chunk_index": r["metadata"].get("chunk_index", 0),
                "score": r["score"]
            }
            for r in retrieved_results
        ]
        yield format_sse_event("citation", {"sources": sources})
```

**æ­¥éª¤ 3**: ä¿®æ”¹ `chat_completions` å‡½æ•°ï¼Œå°†æœç´¢ç»“æœä¼ é€’ç»™ generatorï¼ˆçº¦ç¬¬ 225-285 è¡Œï¼‰ï¼š

```python
# æ‰¾åˆ°è¿™æ®µä»£ç ï¼ˆçº¦ç¬¬ 229-240 è¡Œï¼‰
retrieved_context = None
if request.kb_id:
    try:
        rag_pipeline = get_rag_pipeline()
        results = rag_pipeline.search(request.kb_id, request.message, top_k=5)
        if results:
            context_parts = []
            for i, r in enumerate(results[:3], 1):
                context_parts.append(f"[{i}] {r['text']}")
            retrieved_context = "\n\n".join(context_parts)
    except Exception:
        pass  # Continue without RAG context if retrieval fails

# ä¿®æ”¹ä¸ºï¼ˆä¿å­˜ results å˜é‡ï¼‰
retrieved_results: List[dict] = []
retrieved_context = None
if request.kb_id:
    try:
        rag_pipeline = get_rag_pipeline()
        retrieved_results = rag_pipeline.search(request.kb_id, request.message, top_k=5)
        if retrieved_results:
            context_parts = []
            for i, r in enumerate(retrieved_results[:3], 1):
                context_parts.append(f"[{i}] {r['text']}")
            retrieved_context = "\n\n".join(context_parts)
    except Exception:
        pass

# ä¿®æ”¹ stream_with_save ä¸­çš„è°ƒç”¨ï¼ˆçº¦ç¬¬ 254 è¡Œï¼‰
# ä¿®æ”¹å‰
async for chunk in chat_stream_generator(request, messages_for_llm):

# ä¿®æ”¹å
async for chunk in chat_stream_generator(request, messages_for_llm, retrieved_results):
```

**æ­¥éª¤ 4**: åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ  Optional å¯¼å…¥ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰ï¼š

```python
from typing import AsyncGenerator, List, Optional
```

### éªŒæ”¶æ ‡å‡†
1. å‘é€ä¸€æ¡å¸¦ kb_id çš„èŠå¤©è¯·æ±‚
2. æŸ¥çœ‹æœåŠ¡ç«¯æ—¥å¿—ï¼Œç¡®è®¤ RAG æœç´¢åªæ‰§è¡Œäº†ä¸€æ¬¡
3. ç¡®è®¤å‰ç«¯ä»ç„¶æ”¶åˆ° retrieval æ€ç»´é“¾äº‹ä»¶

---

# Phase 2: æ ¸å¿ƒåŠŸèƒ½ï¼ˆå·¥ä½œæµå¼•æ“ï¼‰

## Task 2.1: åˆ›å»ºå·¥ä½œæµæ‰§è¡Œå¼•æ“æ ¸å¿ƒæ¨¡å—

### ç›®æ ‡
å®ç°å·¥ä½œæµæ‰§è¡Œå¼•æ“ï¼Œæ”¯æŒæ‹“æ‰‘æ’åºæ‰§è¡Œã€å˜é‡ä¼ é€’ã€å„ç±»èŠ‚ç‚¹å¤„ç†ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**åˆ›å»ºæ–°æ–‡ä»¶**: `backend/app/core/workflow_engine.py`

```python
"""
Workflow Execution Engine

Supports:
- Topological sort execution
- Variable passing between nodes
- Condition branching
- Async streaming output
"""

from typing import Any, Dict, List, Optional, AsyncGenerator
from collections import deque
import re

from app.core.llm import chat_completion_stream
from app.core.rag import get_rag_pipeline
from app.models.workflow import Workflow


class ExecutionContext:
    """Execution context for storing variables and intermediate results."""

    def __init__(self, initial_input: str):
        self.variables: Dict[str, Any] = {
            "input": initial_input,
        }
        self.step_outputs: Dict[str, Any] = {}

    def set_output(self, node_id: str, value: Any) -> None:
        """Set output for a node."""
        self.step_outputs[node_id] = value
        self.variables[f"{node_id}.output"] = value

    def get_variable(self, var_path: str) -> Any:
        """
        Resolve variable path like 'step1.output'.

        Args:
            var_path: Variable path (e.g., 'node_1.output')

        Returns:
            Variable value or None if not found
        """
        parts = var_path.split('.')
        current = self.variables
        for part in parts:
            if isinstance(current, dict):
                current = current.get(part)
            else:
                return None
        return current

    def resolve_template(self, template: str) -> str:
        """
        Resolve template string with variable references {{var}}.

        Args:
            template: Template string with {{variable}} placeholders

        Returns:
            Resolved string with variables replaced
        """
        def replace_var(match):
            var_path = match.group(1)
            value = self.get_variable(var_path)
            return str(value) if value is not None else match.group(0)

        return re.sub(r'\{\{(\w+(?:\.\w+)*)\}\}', replace_var, template)


class WorkflowEngine:
    """Workflow execution engine."""

    def __init__(self, workflow: Workflow):
        """
        Initialize the workflow engine.

        Args:
            workflow: Workflow model with graph_data
        """
        self.workflow = workflow
        self.nodes: Dict[str, dict] = {
            n["id"]: n for n in workflow.graph_data.nodes
        }
        self.edges: List[dict] = workflow.graph_data.edges
        self.adjacency = self._build_adjacency()

    def _build_adjacency(self) -> Dict[str, List[str]]:
        """Build adjacency list from edges."""
        adj: Dict[str, List[str]] = {node_id: [] for node_id in self.nodes}
        for edge in self.edges:
            source = edge.get("source")
            target = edge.get("target")
            if source and target and source in adj:
                adj[source].append(target)
        return adj

    def _get_in_edges(self) -> Dict[str, List[str]]:
        """Build reverse adjacency (incoming edges)."""
        in_edges: Dict[str, List[str]] = {node_id: [] for node_id in self.nodes}
        for edge in self.edges:
            source = edge.get("source")
            target = edge.get("target")
            if source and target and target in in_edges:
                in_edges[target].append(source)
        return in_edges

    def _topological_sort(self) -> List[str]:
        """
        Topological sort of nodes using Kahn's algorithm.

        Returns:
            List of node IDs in execution order

        Raises:
            ValueError: If workflow contains cycles
        """
        in_degree = {node_id: 0 for node_id in self.nodes}

        for edge in self.edges:
            target = edge.get("target")
            if target and target in in_degree:
                in_degree[target] += 1

        # Start with nodes that have no incoming edges
        queue = deque([
            node_id for node_id, degree in in_degree.items()
            if degree == 0
        ])
        result = []

        while queue:
            node_id = queue.popleft()
            result.append(node_id)

            for neighbor in self.adjacency.get(node_id, []):
                in_degree[neighbor] -= 1
                if in_degree[neighbor] == 0:
                    queue.append(neighbor)

        if len(result) != len(self.nodes):
            raise ValueError("Workflow contains cycles - cannot execute")

        return result

    def _get_source_nodes(self, node_id: str) -> List[str]:
        """Get all source nodes pointing to this node."""
        sources = []
        for edge in self.edges:
            if edge.get("target") == node_id:
                sources.append(edge.get("source"))
        return [s for s in sources if s]

    def _get_input_for_node(self, node_id: str, ctx: ExecutionContext) -> Any:
        """Get input value for a node from its source nodes."""
        source_nodes = self._get_source_nodes(node_id)

        # If multiple sources, use the first one with output
        for source_id in source_nodes:
            if source_id in ctx.step_outputs:
                return ctx.step_outputs[source_id]

        # Fall back to initial input
        return ctx.variables.get("input", "")

    async def _execute_start_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """Execute start node."""
        node_id = node["id"]

        yield {
            "type": "node_start",
            "node_id": node_id,
            "node_type": "start"
        }

        # Start node passes through the initial input
        data = node.get("data", {})
        input_var = data.get("inputVariable", "input")
        output = ctx.variables.get("input", "")

        ctx.set_output(node_id, output)

        yield {
            "type": "node_complete",
            "node_id": node_id,
            "output": output[:100] + "..." if len(str(output)) > 100 else output
        }

    async def _execute_llm_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """Execute LLM node."""
        node_id = node["id"]

        yield {
            "type": "node_start",
            "node_id": node_id,
            "node_type": "llm"
        }

        data = node.get("data", {})
        system_prompt = data.get("systemPrompt", "You are a helpful assistant.")
        temperature = data.get("temperature", 0.7)

        # Resolve variables in system prompt
        system_prompt = ctx.resolve_template(system_prompt)

        # Get input from source node
        input_text = self._get_input_for_node(node_id, ctx)

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": str(input_text)}
        ]

        yield {
            "type": "thought",
            "node_id": node_id,
            "content": f"Calling LLM with temperature={temperature}"
        }

        output = ""
        try:
            async for token in chat_completion_stream(messages, temperature=temperature):
                output += token
                yield {
                    "type": "token",
                    "node_id": node_id,
                    "content": token
                }
        except Exception as e:
            yield {
                "type": "node_error",
                "node_id": node_id,
                "error": f"LLM call failed: {str(e)}"
            }
            return

        ctx.set_output(node_id, output)

        yield {
            "type": "node_complete",
            "node_id": node_id,
            "output": output[:100] + "..." if len(output) > 100 else output
        }

    async def _execute_knowledge_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """Execute knowledge retrieval node."""
        node_id = node["id"]

        yield {
            "type": "node_start",
            "node_id": node_id,
            "node_type": "knowledge"
        }

        data = node.get("data", {})
        kb_id = data.get("knowledgeBaseId")

        if not kb_id:
            yield {
                "type": "node_error",
                "node_id": node_id,
                "error": "Knowledge base not configured"
            }
            ctx.set_output(node_id, "")
            return

        # Get query from input
        query = str(self._get_input_for_node(node_id, ctx))

        yield {
            "type": "thought",
            "node_id": node_id,
            "content": f"Searching knowledge base: {kb_id}"
        }

        try:
            rag_pipeline = get_rag_pipeline()
            results = rag_pipeline.search(kb_id, query, top_k=5)

            # Format results as context
            context_parts = []
            for i, r in enumerate(results[:3], 1):
                context_parts.append(f"[{i}] {r['text']}")

            output = "\n\n".join(context_parts) if context_parts else "No relevant documents found."
            ctx.set_output(node_id, output)

            yield {
                "type": "retrieval_result",
                "node_id": node_id,
                "results_count": len(results),
                "top_results": [
                    {
                        "text": r["text"][:200] + "..." if len(r["text"]) > 200 else r["text"],
                        "score": r["score"]
                    }
                    for r in results[:3]
                ]
            }

            yield {
                "type": "node_complete",
                "node_id": node_id,
                "output": f"Found {len(results)} relevant chunks"
            }

        except Exception as e:
            yield {
                "type": "node_error",
                "node_id": node_id,
                "error": str(e)
            }
            ctx.set_output(node_id, "")

    async def _execute_condition_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """Execute condition node."""
        node_id = node["id"]

        yield {
            "type": "node_start",
            "node_id": node_id,
            "node_type": "condition"
        }

        data = node.get("data", {})
        expression = data.get("expression", "true")

        # Resolve variables in expression
        resolved_expr = ctx.resolve_template(expression)

        yield {
            "type": "thought",
            "node_id": node_id,
            "content": f"Evaluating: {resolved_expr}"
        }

        # Simple safe evaluation
        result = self._safe_eval_condition(resolved_expr)

        ctx.set_output(node_id, result)

        yield {
            "type": "condition_result",
            "node_id": node_id,
            "expression": resolved_expr,
            "result": result,
            "branch": "true" if result else "false"
        }

        yield {
            "type": "node_complete",
            "node_id": node_id,
            "output": result
        }

    def _safe_eval_condition(self, expression: str) -> bool:
        """
        Safely evaluate a condition expression.
        Only supports simple comparisons, not arbitrary code.
        """
        expression = expression.strip()

        # Handle simple boolean strings
        if expression.lower() in ("true", "yes", "1"):
            return True
        if expression.lower() in ("false", "no", "0", ""):
            return False

        # Handle simple comparisons
        try:
            # Try equality check
            if "===" in expression or "==" in expression:
                parts = expression.replace("===", "==").split("==")
                if len(parts) == 2:
                    left = parts[0].strip().strip("'\"")
                    right = parts[1].strip().strip("'\"")
                    return left == right

            # Try inequality check
            if "!==" in expression or "!=" in expression:
                parts = expression.replace("!==", "!=").split("!=")
                if len(parts) == 2:
                    left = parts[0].strip().strip("'\"")
                    right = parts[1].strip().strip("'\"")
                    return left != right

            # Try contains check
            if " contains " in expression.lower():
                parts = expression.lower().split(" contains ")
                if len(parts) == 2:
                    left = parts[0].strip().strip("'\"")
                    right = parts[1].strip().strip("'\"")
                    return right in left

            # Default to truthy check
            return bool(expression)

        except Exception:
            return False

    async def _execute_end_node(
        self,
        node: dict,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """Execute end node."""
        node_id = node["id"]

        yield {
            "type": "node_start",
            "node_id": node_id,
            "node_type": "end"
        }

        # Collect output from source nodes
        final_output = self._get_input_for_node(node_id, ctx)

        data = node.get("data", {})
        output_var = data.get("outputVariable", "result")

        ctx.set_output(node_id, final_output)
        ctx.variables[output_var] = final_output

        yield {
            "type": "node_complete",
            "node_id": node_id,
            "output": str(final_output)[:100] + "..." if len(str(final_output)) > 100 else final_output
        }

        yield {
            "type": "workflow_complete",
            "final_output": final_output,
            "output_variable": output_var
        }

    async def _execute_node(
        self,
        node_id: str,
        ctx: ExecutionContext
    ) -> AsyncGenerator[dict, None]:
        """Execute a single node based on its type."""
        if node_id not in self.nodes:
            yield {
                "type": "node_error",
                "node_id": node_id,
                "error": f"Node not found: {node_id}"
            }
            return

        node = self.nodes[node_id]
        node_type = node.get("type", "unknown")

        executors = {
            "start": self._execute_start_node,
            "llm": self._execute_llm_node,
            "knowledge": self._execute_knowledge_node,
            "condition": self._execute_condition_node,
            "end": self._execute_end_node,
        }

        executor = executors.get(node_type)
        if executor:
            async for event in executor(node, ctx):
                yield event
        else:
            yield {
                "type": "node_error",
                "node_id": node_id,
                "error": f"Unknown node type: {node_type}"
            }

    async def execute(self, initial_input: str) -> AsyncGenerator[dict, None]:
        """
        Execute the entire workflow.

        Args:
            initial_input: Initial input string

        Yields:
            Execution events including:
            - workflow_start
            - node_start
            - token (LLM output chunks)
            - thought (processing status)
            - retrieval_result (knowledge search results)
            - condition_result (condition evaluation)
            - node_complete
            - node_error
            - workflow_complete
            - workflow_error
        """
        yield {
            "type": "workflow_start",
            "workflow_id": self.workflow.id,
            "workflow_name": self.workflow.name,
            "node_count": len(self.nodes)
        }

        if not self.nodes:
            yield {
                "type": "workflow_error",
                "error": "Workflow has no nodes"
            }
            return

        try:
            execution_order = self._topological_sort()
            ctx = ExecutionContext(initial_input)

            yield {
                "type": "thought",
                "content": f"Execution order: {' -> '.join(execution_order)}"
            }

            for node_id in execution_order:
                async for event in self._execute_node(node_id, ctx):
                    yield event

                    # Stop on error
                    if event.get("type") == "node_error":
                        yield {
                            "type": "workflow_error",
                            "error": event.get("error"),
                            "failed_node": node_id
                        }
                        return

        except ValueError as e:
            yield {
                "type": "workflow_error",
                "error": str(e)
            }
        except Exception as e:
            yield {
                "type": "workflow_error",
                "error": f"Unexpected error: {str(e)}"
            }
```

### éªŒæ”¶æ ‡å‡†
1. æ–‡ä»¶åˆ›å»ºæˆåŠŸ
2. æ—  Python è¯­æ³•é”™è¯¯
3. å¯¼å…¥æµ‹è¯•é€šè¿‡: `python -c "from app.core.workflow_engine import WorkflowEngine"`

---

## Task 2.2: ä¿®æ”¹ Chat API æ”¯æŒå·¥ä½œæµæ‰§è¡Œ

### ç›®æ ‡
å½“è¯·æ±‚åŒ…å« workflow_id æ—¶ï¼Œä½¿ç”¨å·¥ä½œæµå¼•æ“æ‰§è¡Œè€Œéæ™®é€šå¯¹è¯ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/chat.py`

**æ­¥éª¤ 1**: æ·»åŠ å¯¼å…¥ï¼ˆåœ¨æ–‡ä»¶é¡¶éƒ¨ï¼‰ï¼š

```python
from app.core.workflow_engine import WorkflowEngine
from app.api.workflow import get_workflow  # ç¡®ä¿è¿™ä¸ªå‡½æ•°å­˜åœ¨
```

**æ­¥éª¤ 2**: æ·»åŠ å·¥ä½œæµæµå¼å¤„ç†å‡½æ•°ï¼ˆåœ¨ `chat_stream_generator` å‡½æ•°åé¢ï¼‰ï¼š

```python
async def workflow_stream_generator(
    request: ChatRequest,
    session: SessionHistory
) -> AsyncGenerator[str, None]:
    """
    Execute workflow and stream results.

    Args:
        request: Chat request with workflow_id
        session: Session history for saving messages

    Yields:
        SSE formatted events
    """
    # Load workflow
    try:
        from app.api.workflow import load_workflows, workflow_to_model
        data = load_workflows()
        workflows = data.get("workflows", {})

        if request.workflow_id not in workflows:
            yield format_sse_event("error", {
                "message": f"Workflow not found: {request.workflow_id}"
            })
            yield format_sse_event("done", {"status": "error"})
            return

        workflow = workflow_to_model(request.workflow_id, workflows[request.workflow_id])
    except Exception as e:
        yield format_sse_event("error", {
            "message": f"Failed to load workflow: {str(e)}"
        })
        yield format_sse_event("done", {"status": "error"})
        return

    # Create engine and execute
    engine = WorkflowEngine(workflow)
    full_output = ""
    has_error = False

    async for event in engine.execute(request.message):
        event_type = event.get("type", "unknown")

        if event_type == "workflow_start":
            yield format_sse_event("thought", {
                "type": "workflow",
                "status": "start",
                "workflow_name": event.get("workflow_name"),
                "node_count": event.get("node_count")
            })

        elif event_type == "node_start":
            yield format_sse_event("thought", {
                "type": "node",
                "status": "start",
                "node_id": event.get("node_id"),
                "node_type": event.get("node_type")
            })

        elif event_type == "token":
            content = event.get("content", "")
            full_output += content
            yield format_sse_event("token", {"content": content})

        elif event_type == "thought":
            yield format_sse_event("thought", {
                "type": "thinking",
                "node_id": event.get("node_id"),
                "content": event.get("content")
            })

        elif event_type == "retrieval_result":
            yield format_sse_event("thought", {
                "type": "retrieval",
                "status": "complete",
                "results_count": event.get("results_count"),
                "top_results": event.get("top_results")
            })

        elif event_type == "condition_result":
            yield format_sse_event("thought", {
                "type": "condition",
                "expression": event.get("expression"),
                "result": event.get("result"),
                "branch": event.get("branch")
            })

        elif event_type == "node_complete":
            yield format_sse_event("thought", {
                "type": "node",
                "status": "complete",
                "node_id": event.get("node_id")
            })

        elif event_type == "node_error":
            has_error = True
            yield format_sse_event("error", {
                "node_id": event.get("node_id"),
                "message": event.get("error")
            })

        elif event_type == "workflow_complete":
            final_output = event.get("final_output", full_output)

            # Save assistant message
            assistant_message = ChatMessage(
                role="assistant",
                content=str(final_output) if final_output else full_output,
                timestamp=datetime.utcnow()
            )
            session.messages.append(assistant_message)
            save_session(session)

            yield format_sse_event("done", {
                "status": "success",
                "message": "Workflow completed"
            })

        elif event_type == "workflow_error":
            has_error = True
            yield format_sse_event("error", {
                "message": event.get("error"),
                "failed_node": event.get("failed_node")
            })
            yield format_sse_event("done", {
                "status": "error",
                "message": event.get("error")
            })

    # If no explicit done event was sent
    if not has_error and full_output:
        yield format_sse_event("done", {
            "status": "success",
            "message": "Workflow completed"
        })
```

**æ­¥éª¤ 3**: ä¿®æ”¹ `chat_completions` å‡½æ•°æ”¯æŒå·¥ä½œæµåˆ†æ”¯ï¼ˆçº¦ç¬¬ 177 è¡Œï¼‰ï¼š

```python
@router.post("/completions")
async def chat_completions(request: ChatRequest) -> StreamingResponse:
    """
    SSE streaming chat completion endpoint.

    Supports:
    - Simple chat (no workflow_id, no kb_id)
    - RAG-enhanced chat (with kb_id)
    - Workflow execution (with workflow_id)
    """
    if not request.message.strip():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Message cannot be empty"
        )

    # Load or create session
    session = load_session(request.session_id)
    if session is None:
        session = SessionHistory(
            session_id=request.session_id,
            kb_id=request.kb_id,
            workflow_id=request.workflow_id
        )

    # Update session metadata
    if request.kb_id:
        session.kb_id = request.kb_id
    if request.workflow_id:
        session.workflow_id = request.workflow_id

    # Add user message
    user_message = ChatMessage(
        role="user",
        content=request.message,
        timestamp=datetime.utcnow()
    )
    session.messages.append(user_message)

    # Choose execution mode
    if request.workflow_id:
        # Workflow execution mode
        return StreamingResponse(
            workflow_stream_generator(request, session),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    else:
        # Standard RAG/chat mode (existing logic)
        # ... ä¿æŒç°æœ‰çš„ RAG å¯¹è¯é€»è¾‘ä¸å˜ ...

        # Prepare messages for LLM
        messages_for_llm: List[dict] = []

        # RAG retrieval
        retrieved_results: List[dict] = []
        retrieved_context = None
        if request.kb_id:
            try:
                rag_pipeline = get_rag_pipeline()
                retrieved_results = rag_pipeline.search(request.kb_id, request.message, top_k=5)
                if retrieved_results:
                    context_parts = []
                    for i, r in enumerate(retrieved_results[:3], 1):
                        context_parts.append(f"[{i}] {r['text']}")
                    retrieved_context = "\n\n".join(context_parts)
            except Exception:
                pass

        system_prompt = build_system_prompt(bool(request.kb_id), retrieved_context)
        messages_for_llm.append({"role": "system", "content": system_prompt})

        # Add conversation history (last 10 messages)
        for msg in session.messages[-10:]:
            messages_for_llm.append({"role": msg.role, "content": msg.content})

        async def stream_with_save():
            assistant_content = ""

            async for chunk in chat_stream_generator(request, messages_for_llm, retrieved_results):
                if chunk.startswith("event: token"):
                    try:
                        lines = chunk.strip().split("\n")
                        for line in lines:
                            if line.startswith("data: "):
                                data = json.loads(line[6:])
                                assistant_content += data.get("content", "")
                    except (json.JSONDecodeError, IndexError):
                        pass
                yield chunk

            if assistant_content:
                assistant_message = ChatMessage(
                    role="assistant",
                    content=assistant_content,
                    timestamp=datetime.utcnow()
                )
                session.messages.append(assistant_message)
                save_session(session)

        return StreamingResponse(
            stream_with_save(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
```

### éªŒæ”¶æ ‡å‡†
1. å‘é€ä¸å¸¦ workflow_id çš„è¯·æ±‚ï¼Œæ­£å¸¸è¿›è¡Œ RAG å¯¹è¯
2. å‘é€å¸¦ workflow_id çš„è¯·æ±‚ï¼Œæ‰§è¡Œå·¥ä½œæµ
3. å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­èƒ½çœ‹åˆ° thought äº‹ä»¶
4. å·¥ä½œæµæ‰§è¡Œå®Œæˆåæ¶ˆæ¯è¢«ä¿å­˜åˆ°ä¼šè¯

---

## Task 2.3: å‰ç«¯æ·»åŠ å·¥ä½œæµ/çŸ¥è¯†åº“é€‰æ‹©å™¨

### ç›®æ ‡
åœ¨ ChatTerminal ä¸­æ·»åŠ å·¥ä½œæµå’ŒçŸ¥è¯†åº“é€‰æ‹©åŠŸèƒ½ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `frontend/src/views/ChatTerminal.vue`

**æ­¥éª¤ 1**: åœ¨ template çš„ input-area ä¹‹å‰æ·»åŠ é€‰æ‹©å™¨ï¼ˆçº¦ç¬¬ 57 è¡Œå‰ï¼‰ï¼š

```vue
<!-- åœ¨ </div> <!-- messages-container --> ä¹‹åï¼Œ<div class="input-area"> ä¹‹å‰æ·»åŠ  -->

<!-- é…ç½®åŒºåŸŸ -->
<div class="config-bar">
  <div class="config-item">
    <label>å·¥ä½œæµ:</label>
    <select v-model="selectedWorkflowId" :disabled="isStreaming">
      <option value="">æ— ï¼ˆæ™®é€šå¯¹è¯ï¼‰</option>
      <option v-for="wf in workflows" :key="wf.id" :value="wf.id">
        {{ wf.name }}
      </option>
    </select>
  </div>
  <div class="config-item">
    <label>çŸ¥è¯†åº“:</label>
    <select v-model="selectedKbId" :disabled="isStreaming || !!selectedWorkflowId">
      <option value="">æ— </option>
      <option v-for="kb in knowledgeBases" :key="kb.id" :value="kb.id">
        {{ kb.name }}
      </option>
    </select>
  </div>
</div>
```

**æ­¥éª¤ 2**: åœ¨ script setup ä¸­æ·»åŠ çŠ¶æ€å’ŒåŠ è½½å‡½æ•°ï¼ˆçº¦ç¬¬ 100 è¡Œåï¼‰ï¼š

```typescript
// æ–°å¢çŠ¶æ€
const selectedWorkflowId = ref<string>('')
const selectedKbId = ref<string>('')
const workflows = ref<{id: string, name: string}[]>([])
const knowledgeBases = ref<{id: string, name: string}[]>([])

// åŠ è½½å·¥ä½œæµåˆ—è¡¨
async function loadWorkflows() {
  try {
    const response = await axios.get('/api/v1/workflows')
    workflows.value = (response.data.items || []).map((w: any) => ({
      id: w.id,
      name: w.name
    }))
  } catch (error) {
    console.error('åŠ è½½å·¥ä½œæµåˆ—è¡¨å¤±è´¥:', error)
  }
}

// åŠ è½½çŸ¥è¯†åº“åˆ—è¡¨
async function loadKnowledgeBases() {
  try {
    const response = await axios.get('/api/v1/knowledge')
    const items = response.data.items || response.data || []
    knowledgeBases.value = items.map((kb: any) => ({
      id: kb.id,
      name: kb.name
    }))
  } catch (error) {
    console.error('åŠ è½½çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥:', error)
  }
}
```

**æ­¥éª¤ 3**: ä¿®æ”¹ onMounted åŠ è½½æ•°æ®ï¼ˆçº¦ç¬¬ 362 è¡Œï¼‰ï¼š

```typescript
onMounted(() => {
  if (sessions.value.length === 0) {
    createNewSession()
  }
  // åŠ è½½å·¥ä½œæµå’ŒçŸ¥è¯†åº“åˆ—è¡¨
  loadWorkflows()
  loadKnowledgeBases()
})
```

**æ­¥éª¤ 4**: ä¿®æ”¹ connectSSE å‡½æ•°ä¼ é€’å‚æ•°ï¼ˆçº¦ç¬¬ 207-218 è¡Œï¼‰ï¼š

```typescript
async function connectSSE(sessionId: string, message: string) {
  const response = await fetch('/api/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      session_id: sessionId,
      message: message,
      workflow_id: selectedWorkflowId.value || undefined,
      kb_id: selectedKbId.value || undefined,
    }),
  })
  // ... åç»­ä»£ç ä¸å˜
}
```

**æ­¥éª¤ 5**: åœ¨ style scoped ä¸­æ·»åŠ æ ·å¼ï¼ˆæ–‡ä»¶æœ«å°¾ï¼‰ï¼š

```css
/* é…ç½®æ  */
.config-bar {
  display: flex;
  gap: 20px;
  padding: 12px 20px;
  background-color: #f8f9fa;
  border-top: 1px solid #e0e0e0;
}

.config-item {
  display: flex;
  align-items: center;
  gap: 8px;
}

.config-item label {
  font-size: 13px;
  color: #666;
  white-space: nowrap;
}

.config-item select {
  padding: 6px 12px;
  border: 1px solid #ddd;
  border-radius: 6px;
  font-size: 13px;
  background-color: white;
  min-width: 150px;
  cursor: pointer;
}

.config-item select:disabled {
  background-color: #f5f5f5;
  cursor: not-allowed;
}

.config-item select:focus {
  outline: none;
  border-color: #2c3e50;
}
```

### éªŒæ”¶æ ‡å‡†
1. é¡µé¢åŠ è½½åèƒ½çœ‹åˆ°å·¥ä½œæµå’ŒçŸ¥è¯†åº“é€‰æ‹©å™¨
2. é€‰æ‹©å™¨èƒ½æ­£ç¡®åŠ è½½åç«¯æ•°æ®
3. é€‰æ‹©å·¥ä½œæµåå‘é€æ¶ˆæ¯ï¼Œè¯·æ±‚ä¸­åŒ…å« workflow_id
4. é€‰æ‹©çŸ¥è¯†åº“åå‘é€æ¶ˆæ¯ï¼Œè¯·æ±‚ä¸­åŒ…å« kb_id

---

## Task 2.4: å¤„ç†å·¥ä½œæµæ‰§è¡Œçš„å‰ç«¯äº‹ä»¶

### ç›®æ ‡
åœ¨ ChatTerminal ä¸­æ­£ç¡®å¤„ç†å·¥ä½œæµæ‰§è¡Œçš„å„ç§ SSE äº‹ä»¶ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `frontend/src/views/ChatTerminal.vue`

**ä¿®æ”¹ handleSSEEvent å‡½æ•°ï¼ˆçº¦ç¬¬ 273-330 è¡Œï¼‰**ï¼š

```typescript
function handleSSEEvent(eventType: string, data: any, lastMessage: Message | undefined) {
  if (!lastMessage || lastMessage.role !== 'assistant') return

  switch (eventType) {
    case 'thought':
      // å¤„ç†å„ç±»æ€ç»´é“¾äº‹ä»¶
      if (data.type === 'workflow') {
        if (data.status === 'start') {
          currentThought.value = `ğŸš€ å¼€å§‹æ‰§è¡Œå·¥ä½œæµ: ${data.workflow_name}`
        }
      } else if (data.type === 'node') {
        if (data.status === 'start') {
          const nodeLabels: Record<string, string> = {
            start: 'å¼€å§‹',
            llm: 'LLM',
            knowledge: 'çŸ¥è¯†åº“',
            condition: 'æ¡ä»¶',
            end: 'ç»“æŸ'
          }
          const label = nodeLabels[data.node_type] || data.node_type
          currentThought.value = `âš™ï¸ æ‰§è¡ŒèŠ‚ç‚¹: ${label}`
        } else if (data.status === 'complete') {
          currentThought.value = `âœ… èŠ‚ç‚¹å®Œæˆ: ${data.node_id}`
        }
      } else if (data.type === 'retrieval') {
        if (data.status === 'start') {
          currentThought.value = 'ğŸ” æ­£åœ¨æ£€ç´¢çŸ¥è¯†åº“...'
        } else if (data.status === 'searching') {
          currentThought.value = 'ğŸ” æ­£åœ¨æœç´¢ç›¸å…³æ–‡æ¡£...'
        } else if (data.status === 'complete') {
          const count = data.results_count || 0
          currentThought.value = `ğŸ“š æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° ${count} ä¸ªç›¸å…³ç‰‡æ®µ`
          setTimeout(() => {
            if (currentThought.value.includes('æ£€ç´¢å®Œæˆ')) {
              currentThought.value = ''
            }
          }, 2000)
        } else if (data.status === 'error') {
          currentThought.value = 'âŒ æ£€ç´¢å‡ºé”™: ' + (data.error || 'æœªçŸ¥é”™è¯¯')
        }
      } else if (data.type === 'condition') {
        currentThought.value = `ğŸ”€ æ¡ä»¶åˆ¤æ–­: ${data.expression} â†’ ${data.branch}`
      } else if (data.type === 'thinking') {
        currentThought.value = `ğŸ’­ ${data.content}`
      } else if (data.content) {
        currentThought.value = data.content
      }
      break

    case 'token':
      // æ‰“å­—æœºæ•ˆæœ
      lastMessage.content += data.content
      scrollToBottom()
      break

    case 'citation':
      // å¼•ç”¨æ¥æº
      if (data.sources && Array.isArray(data.sources)) {
        const citations = data.sources.map((s: any, i: number) =>
          `[å¼•ç”¨${i + 1}] doc:${s.doc_id?.slice(0, 8)}, score:${(s.score || 0).toFixed(2)}`
        ).join('\n')
        // ä¸ç›´æ¥è¿½åŠ åˆ°æ¶ˆæ¯ï¼Œå¯ä»¥å­˜å‚¨èµ·æ¥ç”¨äºæ˜¾ç¤º
        console.log('Citations:', citations)
      }
      break

    case 'error':
      // é”™è¯¯å¤„ç†
      const errorMsg = data.message || data.content || 'æœªçŸ¥é”™è¯¯'
      lastMessage.content += `\n\nâŒ é”™è¯¯: ${errorMsg}`
      currentThought.value = `âŒ ${errorMsg}`
      break

    case 'done':
      // å®Œæˆ
      isStreaming.value = false
      lastMessage.isStreaming = false
      if (data.status === 'success') {
        currentThought.value = ''
      } else {
        currentThought.value = `âš ï¸ ${data.message || 'æ‰§è¡Œç»“æŸ'}`
      }
      break

    default:
      console.log('Unknown SSE event:', eventType, data)
  }
}
```

### éªŒæ”¶æ ‡å‡†
1. æ‰§è¡Œå·¥ä½œæµæ—¶èƒ½çœ‹åˆ°èŠ‚ç‚¹æ‰§è¡Œçš„çŠ¶æ€æç¤º
2. LLM èŠ‚ç‚¹çš„è¾“å‡ºèƒ½æ­£ç¡®æ˜¾ç¤ºæ‰“å­—æœºæ•ˆæœ
3. é”™è¯¯èƒ½æ­£ç¡®æ˜¾ç¤º
4. å·¥ä½œæµå®ŒæˆåçŠ¶æ€æ­£ç¡®é‡ç½®

---

## Task 2.5: æ·»åŠ å·¥ä½œæµæ‰§è¡Œ API ç«¯ç‚¹

### ç›®æ ‡
æ·»åŠ ä¸€ä¸ªç‹¬ç«‹çš„å·¥ä½œæµæµ‹è¯•æ‰§è¡Œç«¯ç‚¹ï¼ˆéèŠå¤©æ¨¡å¼ï¼‰ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/workflow.py`

**åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ **ï¼š

```python
from fastapi.responses import StreamingResponse
from app.core.workflow_engine import WorkflowEngine
import json


def format_sse(event: str, data: dict) -> str:
    """Format SSE event."""
    return f"event: {event}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"


@router.post("/{workflow_id}/execute")
async def execute_workflow(
    workflow_id: str,
    input_data: dict
) -> StreamingResponse:
    """
    Execute a workflow with given input.

    This endpoint is for testing workflows directly without going through chat.

    - **workflow_id**: Workflow ID to execute
    - **input_data**: JSON body with 'input' field containing the initial input

    Returns SSE stream of execution events.
    """
    data = load_workflows()
    workflows = data.get("workflows", {})

    if workflow_id not in workflows:
        raise HTTPException(status_code=404, detail=f"Workflow {workflow_id} not found")

    workflow = workflow_to_model(workflow_id, workflows[workflow_id])
    initial_input = input_data.get("input", "")

    if not initial_input:
        raise HTTPException(status_code=400, detail="Input cannot be empty")

    async def generate():
        engine = WorkflowEngine(workflow)

        async for event in engine.execute(initial_input):
            event_type = event.pop("type", "unknown")
            yield format_sse(event_type, event)

        yield format_sse("done", {"status": "complete"})

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```

### éªŒæ”¶æ ‡å‡†
1. å¯ä»¥é€šè¿‡ `POST /api/v1/workflows/{id}/execute` æ‰§è¡Œå·¥ä½œæµ
2. è¿”å› SSE æ ¼å¼çš„æ‰§è¡Œäº‹ä»¶æµ
3. Swagger æ–‡æ¡£ä¸­èƒ½çœ‹åˆ°è¯¥ç«¯ç‚¹

---

## Task 2.6: ä¿®å¤å·¥ä½œæµ API å¯¼å…¥ä¾èµ–

### ç›®æ ‡
ç¡®ä¿ workflow.py ä¸­å¯ä»¥æ­£ç¡®å¯¼å…¥ workflow_engineã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/workflow.py`

**åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ æ¡ä»¶å¯¼å…¥ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰**ï¼š

```python
# åœ¨ç°æœ‰å¯¼å…¥ä¹‹åæ·»åŠ 
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from app.core.workflow_engine import WorkflowEngine
```

**åœ¨ execute_workflow å‡½æ•°å†…éƒ¨è¿›è¡Œå®é™…å¯¼å…¥**ï¼š

```python
@router.post("/{workflow_id}/execute")
async def execute_workflow(
    workflow_id: str,
    input_data: dict
) -> StreamingResponse:
    # åœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
    from app.core.workflow_engine import WorkflowEngine

    # ... å‡½æ•°å…¶ä½™éƒ¨åˆ†
```

### éªŒæ”¶æ ‡å‡†
1. æœåŠ¡å™¨å¯åŠ¨æ— å¯¼å…¥é”™è¯¯
2. æ‰§è¡Œå·¥ä½œæµç«¯ç‚¹æ­£å¸¸å·¥ä½œ

---

# Phase 3: åŠŸèƒ½å®Œå–„

## Task 3.1: æ·»åŠ çŸ¥è¯†åº“åˆ é™¤ API

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/knowledge.py`

**åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ **ï¼š

```python
import shutil


@router.delete("/{kb_id}", status_code=204)
async def delete_knowledge_base(kb_id: str) -> None:
    """
    Delete a knowledge base and all its documents.

    This will:
    1. Delete the ChromaDB collection
    2. Delete all uploaded files
    3. Delete all metadata
    4. Remove from knowledge base index

    - **kb_id**: Knowledge base ID to delete
    """
    # Check if knowledge base exists
    metadata = load_kb_metadata()
    if kb_id not in metadata:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Knowledge base '{kb_id}' not found"
        )

    project_root = Path(__file__).parent.parent.parent

    # 1. Delete ChromaDB collection
    try:
        chroma_client = get_chroma_client()
        chroma_client.delete_collection(kb_id)
    except Exception as e:
        print(f"Warning: Failed to delete ChromaDB collection: {e}")

    # 2. Delete uploaded files
    upload_dir = project_root / "data" / "uploads" / kb_id
    if upload_dir.exists():
        try:
            shutil.rmtree(upload_dir)
        except Exception as e:
            print(f"Warning: Failed to delete upload directory: {e}")

    # 3. Delete metadata directory
    metadata_dir = project_root / "data" / "metadata" / kb_id
    if metadata_dir.exists():
        try:
            shutil.rmtree(metadata_dir)
        except Exception as e:
            print(f"Warning: Failed to delete metadata directory: {e}")

    # 4. Remove from index
    del metadata[kb_id]
    save_kb_metadata(metadata)
```

### éªŒæ”¶æ ‡å‡†
1. å¯ä»¥é€šè¿‡ `DELETE /api/v1/knowledge/{kb_id}` åˆ é™¤çŸ¥è¯†åº“
2. åˆ é™¤å ChromaDB collection ä¸å­˜åœ¨
3. åˆ é™¤åä¸Šä¼ æ–‡ä»¶å¤¹è¢«æ¸…ç†
4. åˆ é™¤åå…ƒæ•°æ®è¢«æ¸…ç†

---

## Task 3.2: ä¿®å¤ç›¸ä¼¼åº¦åˆ†æ•°è®¡ç®—

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/core/rag.py`

**ä¿®æ”¹ search æ–¹æ³•ä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆçº¦ç¬¬ 180-194 è¡Œï¼‰**ï¼š

```python
# ä¿®æ”¹å‰
distance = results["distances"][0][i] if results["distances"] else 0
similarity = max(0, 1 - distance)

# ä¿®æ”¹å
distance = results["distances"][0][i] if results["distances"] else 0
# ChromaDB uses L2 distance by default
# For normalized vectors, L2 distance ranges from 0 to 2
# Convert to similarity score using inverse relationship
# This formula gives: distance=0 -> similarity=1, distance=2 -> similarity=0.33
similarity = 1 / (1 + distance)
```

### éªŒæ”¶æ ‡å‡†
1. ç›¸ä¼¼åº¦åˆ†æ•°åœ¨ 0-1 èŒƒå›´å†…
2. è·ç¦»è¶Šè¿‘ï¼Œç›¸ä¼¼åº¦è¶Šé«˜
3. ä¸ä¼šå‡ºç°è´Ÿæ•°åˆ†æ•°

---

## Task 3.3: æ”¹è¿›ä¼šè¯åˆ—è¡¨ API

### ç›®æ ‡
æ·»åŠ è·å–ä¼šè¯åˆ—è¡¨çš„ APIï¼Œä»¥ä¾¿å‰ç«¯å¯ä»¥æ¢å¤ä¼šè¯ã€‚

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/api/chat.py`

**åœ¨ router å®šä¹‰åæ·»åŠ **ï¼š

```python
@router.get("/sessions")
async def list_sessions(
    limit: int = 20,
    offset: int = 0
) -> dict:
    """
    List all chat sessions.

    - **limit**: Maximum number of sessions to return (default: 20)
    - **offset**: Number of sessions to skip (default: 0)
    """
    sessions_list = []

    if SESSIONS_DIR.exists():
        session_files = sorted(
            SESSIONS_DIR.glob("*.json"),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )

        for session_file in session_files[offset:offset + limit]:
            try:
                session_id = session_file.stem
                session = load_session(session_id)
                if session:
                    # Get title from first user message or use default
                    title = "æ–°ä¼šè¯"
                    for msg in session.messages:
                        if msg.role == "user":
                            title = msg.content[:30] + ("..." if len(msg.content) > 30 else "")
                            break

                    sessions_list.append({
                        "session_id": session.session_id,
                        "title": title,
                        "message_count": len(session.messages),
                        "created_at": session.created_at.isoformat(),
                        "updated_at": session.updated_at.isoformat() if session.updated_at else None,
                        "kb_id": session.kb_id,
                        "workflow_id": session.workflow_id
                    })
            except Exception:
                continue

    return {
        "sessions": sessions_list,
        "total": len(list(SESSIONS_DIR.glob("*.json"))) if SESSIONS_DIR.exists() else 0,
        "limit": limit,
        "offset": offset
    }
```

### éªŒæ”¶æ ‡å‡†
1. `GET /api/v1/chat/sessions` è¿”å›ä¼šè¯åˆ—è¡¨
2. åˆ—è¡¨æŒ‰æ›´æ–°æ—¶é—´å€’åºæ’åˆ—
3. åŒ…å«ä¼šè¯æ ‡é¢˜å’Œæ¶ˆæ¯æ•°é‡

---

## Task 3.4: é˜²æ­¢æ–‡ä»¶è¦†ç›–

### æ‰§è¡ŒæŒ‡ä»¤

æ­¤ä»»åŠ¡å·²åœ¨ Task 1.3 ä¸­å®Œæˆï¼ˆä½¿ç”¨ UUID å‰ç¼€ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼‰ã€‚

### éªŒæ”¶æ ‡å‡†
1. ä¸Šä¼ åŒåæ–‡ä»¶ä¸ä¼šè¦†ç›–å·²æœ‰æ–‡ä»¶
2. æ¯ä¸ªæ–‡ä»¶æœ‰å”¯ä¸€çš„å­˜å‚¨åç§°

---

# Phase 4: ç¨³å®šæ€§ä¼˜åŒ–

## Task 4.1: æ·»åŠ  JSON æ–‡ä»¶å¹¶å‘é”

### æ‰§è¡ŒæŒ‡ä»¤

**æ­¥éª¤ 1**: å®‰è£… filelock ä¾èµ–

```bash
cd backend
uv add filelock
# æˆ–è€…
pip install filelock
```

**æ­¥éª¤ 2**: ä¿®æ”¹ `backend/app/api/workflow.py`

```python
# åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥
from filelock import FileLock

# ä¿®æ”¹ load_workflows å‡½æ•°
def load_workflows() -> dict:
    """Load workflows from JSON file with file locking."""
    ensure_data_dir()
    lock = FileLock(str(WORKFLOW_FILE) + ".lock", timeout=10)

    with lock:
        if not WORKFLOW_FILE.exists():
            return {"workflows": {}}
        try:
            with open(WORKFLOW_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {"workflows": {}}


# ä¿®æ”¹ save_workflows å‡½æ•°
def save_workflows(data: dict) -> None:
    """Save workflows to JSON file with file locking."""
    ensure_data_dir()
    lock = FileLock(str(WORKFLOW_FILE) + ".lock", timeout=10)

    with lock:
        with open(WORKFLOW_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
```

**æ­¥éª¤ 3**: å¯¹ `backend/app/api/knowledge.py` è¿›è¡Œç±»ä¼¼ä¿®æ”¹

```python
from filelock import FileLock

def load_kb_metadata() -> dict:
    lock = FileLock(str(KB_METADATA_FILE) + ".lock", timeout=10)
    with lock:
        if not KB_METADATA_FILE.exists():
            return {}
        try:
            with open(KB_METADATA_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}


def save_kb_metadata(metadata: dict) -> None:
    KB_METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)
    lock = FileLock(str(KB_METADATA_FILE) + ".lock", timeout=10)
    with lock:
        with open(KB_METADATA_FILE, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)


def load_documents_metadata(kb_id: str) -> dict:
    metadata_path = get_metadata_path(kb_id)
    lock = FileLock(str(metadata_path) + ".lock", timeout=10)
    with lock:
        if not metadata_path.exists():
            return {}
        try:
            with open(metadata_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}


def save_documents_metadata(kb_id: str, metadata: dict) -> None:
    metadata_path = get_metadata_path(kb_id)
    lock = FileLock(str(metadata_path) + ".lock", timeout=10)
    with lock:
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
```

### éªŒæ”¶æ ‡å‡†
1. å¹¶å‘è¯·æ±‚ä¸ä¼šå¯¼è‡´æ•°æ®ä¸¢å¤±
2. é”æ–‡ä»¶è¶…æ—¶åèƒ½æ­£ç¡®é‡Šæ”¾

---

## Task 4.2: ä¿®å¤å…¨å±€å•ä¾‹çº¿ç¨‹å®‰å…¨

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `backend/app/core/rag.py`

```python
# ä¿®æ”¹æ–‡ä»¶æœ«å°¾çš„å•ä¾‹å®ç°
import threading

_rag_pipeline: Optional[RAGPipeline] = None
_rag_pipeline_lock = threading.Lock()


def get_rag_pipeline() -> RAGPipeline:
    """Get the global RAG pipeline instance (thread-safe)."""
    global _rag_pipeline

    if _rag_pipeline is None:
        with _rag_pipeline_lock:
            # Double-checked locking
            if _rag_pipeline is None:
                _rag_pipeline = RAGPipeline()

    return _rag_pipeline
```

**æ–‡ä»¶**: `backend/app/core/chroma_client.py`

```python
# ä¿®æ”¹æ–‡ä»¶æœ«å°¾çš„å•ä¾‹å®ç°
import threading

_chroma_client: Optional[ChromaClient] = None
_chroma_client_lock = threading.Lock()


def get_chroma_client() -> ChromaClient:
    """Get the global ChromaDB client instance (thread-safe)."""
    global _chroma_client

    if _chroma_client is None:
        with _chroma_client_lock:
            if _chroma_client is None:
                _chroma_client = ChromaClient()

    return _chroma_client
```

### éªŒæ”¶æ ‡å‡†
1. é«˜å¹¶å‘ä¸‹ä¸ä¼šåˆ›å»ºå¤šä¸ªå®ä¾‹
2. æ—  race condition é”™è¯¯

---

## Task 4.3: æ”¹è¿› Session ID ç”Ÿæˆ

### æ‰§è¡ŒæŒ‡ä»¤

**æ–‡ä»¶**: `frontend/src/views/ChatTerminal.vue`

```typescript
// ä¿®æ”¹ generateId å‡½æ•°
function generateId(): string {
  // ä½¿ç”¨ crypto API ç”Ÿæˆå®‰å…¨çš„éšæœº ID
  const array = new Uint8Array(16)
  crypto.getRandomValues(array)
  return Array.from(array, byte => byte.toString(16).padStart(2, '0')).join('')
}
```

### éªŒæ”¶æ ‡å‡†
1. ç”Ÿæˆçš„ ID é•¿åº¦ä¸º 32 ä¸ªåå…­è¿›åˆ¶å­—ç¬¦
2. ID ä¸å¯é¢„æµ‹

---

# æ‰§è¡Œæ£€æŸ¥æ¸…å•

## Phase 1 å®Œæˆæ£€æŸ¥
- [ ] Task 1.1: ä¿å­˜å·¥ä½œæµæ—¶ data å­—æ®µè¢«æ­£ç¡®ä¿å­˜
- [ ] Task 1.2: åˆ é™¤æ–‡æ¡£åå‘é‡æ•°æ®è¢«æ¸…ç†
- [ ] Task 1.3: æ— æ³•ä¸Šä¼ å¸¦è·¯å¾„éå†çš„æ–‡ä»¶å
- [ ] Task 1.4: RAG æœç´¢åªæ‰§è¡Œä¸€æ¬¡

## Phase 2 å®Œæˆæ£€æŸ¥
- [ ] Task 2.1: workflow_engine.py åˆ›å»ºæˆåŠŸ
- [ ] Task 2.2: Chat API æ”¯æŒ workflow_id å‚æ•°
- [ ] Task 2.3: å‰ç«¯æœ‰å·¥ä½œæµ/çŸ¥è¯†åº“é€‰æ‹©å™¨
- [ ] Task 2.4: å·¥ä½œæµæ‰§è¡Œäº‹ä»¶æ­£ç¡®æ˜¾ç¤º
- [ ] Task 2.5: /execute ç«¯ç‚¹å¯ç”¨
- [ ] Task 2.6: æ— å¾ªç¯å¯¼å…¥é”™è¯¯

## Phase 3 å®Œæˆæ£€æŸ¥
- [ ] Task 3.1: çŸ¥è¯†åº“åˆ é™¤ API å¯ç”¨
- [ ] Task 3.2: ç›¸ä¼¼åº¦åˆ†æ•°åˆç†
- [ ] Task 3.3: ä¼šè¯åˆ—è¡¨ API å¯ç”¨
- [ ] Task 3.4: æ–‡ä»¶ä¸ä¼šè¢«è¦†ç›–

## Phase 4 å®Œæˆæ£€æŸ¥
- [ ] Task 4.1: JSON æ–‡ä»¶æ“ä½œæœ‰é”ä¿æŠ¤
- [ ] Task 4.2: å•ä¾‹æ¨¡å¼çº¿ç¨‹å®‰å…¨
- [ ] Task 4.3: Session ID ä½¿ç”¨ crypto API

---

# æµ‹è¯•è„šæœ¬

## æµ‹è¯•å·¥ä½œæµæ‰§è¡Œ

```bash
# 1. åˆ›å»ºæµ‹è¯•å·¥ä½œæµ
curl -X POST http://localhost:8000/api/v1/workflows \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Workflow",
    "description": "A simple test workflow",
    "graph_data": {
      "nodes": [
        {"id": "1", "type": "start", "position": {"x": 100, "y": 100}, "data": {}},
        {"id": "2", "type": "llm", "position": {"x": 300, "y": 100}, "data": {"systemPrompt": "You are a helpful assistant. Be concise.", "temperature": 0.7}},
        {"id": "3", "type": "end", "position": {"x": 500, "y": 100}, "data": {}}
      ],
      "edges": [
        {"id": "e1-2", "source": "1", "target": "2"},
        {"id": "e2-3", "source": "2", "target": "3"}
      ]
    }
  }'

# è®°å½•è¿”å›çš„ workflow_id

# 2. æ‰§è¡Œå·¥ä½œæµ
curl -X POST http://localhost:8000/api/v1/workflows/{workflow_id}/execute \
  -H "Content-Type: application/json" \
  -d '{"input": "What is 2+2?"}' \
  --no-buffer

# 3. é€šè¿‡ Chat API æ‰§è¡Œå·¥ä½œæµ
curl -X POST http://localhost:8000/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "test-session-1",
    "message": "Hello, how are you?",
    "workflow_id": "{workflow_id}"
  }' \
  --no-buffer
```

## æµ‹è¯•çŸ¥è¯†åº“åˆ é™¤

```bash
# 1. åˆ›å»ºçŸ¥è¯†åº“
curl -X POST http://localhost:8000/api/v1/knowledge \
  -H "Content-Type: application/json" \
  -d '{"name": "Test KB"}'

# è®°å½•è¿”å›çš„ kb_id

# 2. ä¸Šä¼ æ–‡æ¡£
curl -X POST http://localhost:8000/api/v1/knowledge/{kb_id}/upload \
  -F "file=@test.txt"

# 3. ç­‰å¾…å¤„ç†å®Œæˆï¼Œæœç´¢æµ‹è¯•
curl "http://localhost:8000/api/v1/knowledge/{kb_id}/search?query=test"

# 4. åˆ é™¤çŸ¥è¯†åº“
curl -X DELETE http://localhost:8000/api/v1/knowledge/{kb_id}

# 5. ç¡®è®¤åˆ é™¤æˆåŠŸ
curl http://localhost:8000/api/v1/knowledge
```
